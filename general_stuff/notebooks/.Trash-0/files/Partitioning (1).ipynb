{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a5aa9e6-65d7-4937-a72b-40a8f70fb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:02:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName(\"iceberg-spark\")\n",
    "\n",
    "        # Spark + Iceberg JAR already in image, no need for jars.packages\n",
    "        .set(\"spark.jars\", \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.9.0-SNAPSHOT.jar\")\n",
    "\n",
    "        # Iceberg extensions\n",
    "        .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "        # REST catalog setup\n",
    "        .set(\"spark.sql.catalog.rest\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.rest.type\", \"rest\")\n",
    "        .set(\"spark.sql.catalog.rest.uri\", \"http://iceberg-rest:8181\")\n",
    "        .set(\"spark.sql.catalog.rest.warehouse\", \"s3://warehouse\")\n",
    "        .set(\"spark.sql.catalog.rest.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.endpoint\", \"http://minio:9000\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.path-style-access\", \"true\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
    "        .set(\"spark.sql.catalog.rest.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
    "\n",
    "        # NESSIE catalog config\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', 'http://minio:9000')\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        #MINIO CREDENTIALS\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "print(\"✅ Spark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181348ea-d56b-47bb-b453-e16b670fb9ec",
   "metadata": {},
   "source": [
    "# partitioning by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e15f31-d934-4d60-bb77-42b32556b356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "721a8d83-ffe5-4b46-8469-088a4f7a8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:08:18 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}\n",
      "25/04/28 17:08:18 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "25/04/28 17:08:18 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}', expected commit-id was 'f4d0c03304e3a63565dadc77670e9a1f1893673c86b65cfd65b001fa40f38a99'\n",
      "25/04/28 17:08:18 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 16 ms\n",
      "25/04/28 17:08:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json\n",
      "25/04/28 17:08:18 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json' at 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.default.evolve_test (\n",
    "    id BIGINT,\n",
    "    ts TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(ts))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b290460-144a-4940-b824-fc9944467a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-04-28 17:29:...|3474037751171505592|               NULL|   append|s3a://warehouse/d...|{spark.app.id -> ...|\n",
      "|2025-04-28 17:42:...|2089434004966214081|3474037751171505592|   append|s3a://warehouse/d...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:43:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:43:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:43:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test.snapshots\n",
      "25/04/28 17:43:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:43:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:43:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:43:33 INFO V2ScanRelationPushDown: \n",
      "Output: committed_at#554, snapshot_id#555L, parent_id#556L, operation#557, manifest_list#558, summary#559\n",
      "         \n",
      "25/04/28 17:43:33 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.snapshots\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 46 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 47 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 9273cf1cd8b1:40087 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Got job 17 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[72] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 15.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 9273cf1cd8b1:40087 (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[72] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:43:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 72) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 14932 bytes) \n",
      "25/04/28 17:43:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 72)\n",
      "25/04/28 17:43:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 72). 5017 bytes result sent to driver\n",
      "25/04/28 17:43:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 72) in 4 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:43:33 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Job 17 finished: showString at NativeMethodAccessorImpl.java:0, took 0.018440 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie.default.evolve_test.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d914f8ea-eb41-4ad0-abfa-12281919d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+\n",
      "|      col_name| data_type|comment|\n",
      "+--------------+----------+-------+\n",
      "|            id|    bigint|   NULL|\n",
      "|            ts| timestamp|   NULL|\n",
      "|              |          |       |\n",
      "|# Partitioning|          |       |\n",
      "|        Part 0|  days(ts)|       |\n",
      "|        Part 1|months(ts)|       |\n",
      "+--------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:40:14 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json\n",
      "25/04/28 17:40:14 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json' at 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}'\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table nessie.default.evolve_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac310ac2-b018-430a-a2a4-abb57b43fdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json\n",
      "25/04/28 17:29:42 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json' at 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}'\n",
      "25/04/28 17:29:42 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 5.348041 ms\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Registering RDD 51 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Got map stage job 11 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.1 KiB, free 434.2 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.2 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Adding task set 11.0 with 12 tasks resource profile 0\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 44) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 45) (9273cf1cd8b1, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 46) (9273cf1cd8b1, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 47) (9273cf1cd8b1, executor driver, partition 3, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 48) (9273cf1cd8b1, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 49) (9273cf1cd8b1, executor driver, partition 5, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 50) (9273cf1cd8b1, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 51) (9273cf1cd8b1, executor driver, partition 7, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 52) (9273cf1cd8b1, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 53) (9273cf1cd8b1, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 54) (9273cf1cd8b1, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 55) (9273cf1cd8b1, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 0.0 in stage 11.0 (TID 44)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 1.0 in stage 11.0 (TID 45)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 3.0 in stage 11.0 (TID 47)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 2.0 in stage 11.0 (TID 46)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 4.0 in stage 11.0 (TID 48)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 5.0 in stage 11.0 (TID 49)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 6.0 in stage 11.0 (TID 50)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 7.0 in stage 11.0 (TID 51)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 8.0 in stage 11.0 (TID 52)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 9.0 in stage 11.0 (TID 53)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 10.0 in stage 11.0 (TID 54)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 11.0 in stage 11.0 (TID 55)\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 5.148791 ms\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 7.418458 ms\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 289, boot = 241, init = 48, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 243, init = 46, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 244, init = 45, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 291, boot = 248, init = 43, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 298, boot = 253, init = 45, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 298, boot = 255, init = 42, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 315, boot = 273, init = 42, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 310, boot = 266, init = 44, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 313, boot = 270, init = 43, finish = 0\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 1.0 in stage 11.0 (TID 45). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 8.0 in stage 11.0 (TID 52). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 10.0 in stage 11.0 (TID 54). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 6.0 in stage 11.0 (TID 50). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 0.0 in stage 11.0 (TID 44). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 9.0 in stage 11.0 (TID 53). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 2.0 in stage 11.0 (TID 46). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 50) in 383 ms on 9273cf1cd8b1 (executor driver) (1/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 45) in 383 ms on 9273cf1cd8b1 (executor driver) (2/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 52) in 382 ms on 9273cf1cd8b1 (executor driver) (3/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 44) in 386 ms on 9273cf1cd8b1 (executor driver) (4/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 46) in 384 ms on 9273cf1cd8b1 (executor driver) (5/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 54) in 383 ms on 9273cf1cd8b1 (executor driver) (6/12)\n",
      "25/04/28 17:29:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37763\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 5.0 in stage 11.0 (TID 49). 2251 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 4.0 in stage 11.0 (TID 48). 2251 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 53) in 386 ms on 9273cf1cd8b1 (executor driver) (7/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 49) in 387 ms on 9273cf1cd8b1 (executor driver) (8/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 48) in 387 ms on 9273cf1cd8b1 (executor driver) (9/12)\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 246, init = 43, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 305, boot = 259, init = 45, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 295, boot = 250, init = 44, finish = 1\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 3.0 in stage 11.0 (TID 47). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 7.0 in stage 11.0 (TID 51). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 11.0 in stage 11.0 (TID 55). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 47) in 409 ms on 9273cf1cd8b1 (executor driver) (10/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 51) in 408 ms on 9273cf1cd8b1 (executor driver) (11/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 55) in 409 ms on 9273cf1cd8b1 (executor driver) (12/12)\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:29:42 INFO DAGScheduler: ShuffleMapStage 11 (append at NativeMethodAccessorImpl.java:0) finished in 0.436 s\n",
      "25/04/28 17:29:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 17:29:42 INFO DAGScheduler: running: Set()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: waiting: Set()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 17:29:42 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 9273cf1cd8b1:40087 in memory (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 35 from broadcast at SparkWrite.java:193\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/28 17:29:42 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Got job 12 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Final stage: ResultStage 13 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting ResultStage 13 (ShuffledRowRDD[52] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 10.4 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (ShuffledRowRDD[52] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 56) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 17:29:42 INFO Executor: Running task 0.0 in stage 13.0 (TID 56)\n",
      "25/04/28 17:29:42 INFO ShuffleBlockFetcherIterator: Getting 3 (198.0 B) non-empty blocks including 3 (198.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 17:29:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 17:29:42 INFO DataWritingSparkTask: Committed partition 0 (task 56, attempt 0, stage 13.0)\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 0.0 in stage 13.0 (TID 56). 8608 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 56) in 94 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:29:42 INFO DAGScheduler: ResultStage 13 (append at NativeMethodAccessorImpl.java:0) finished in 0.109 s\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Job 12 finished: append at NativeMethodAccessorImpl.java:0, took 0.115736 s\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 17:29:42 INFO SparkWrite: Committing append with 3 new data files to table nessie.default.evolve_test\n",
      "25/04/28 17:29:42 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949}', expected commit-id was '62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9'\n",
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 11 ms\n",
      "25/04/28 17:29:42 INFO SnapshotProducer: Committed snapshot 3474037751171505592 (MergeAppend)\n",
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00001-fc9390f6-1bf5-47a1-a6fe-5c01595afcc5.metadata.json\n",
      "25/04/28 17:29:42 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00001-fc9390f6-1bf5-47a1-a6fe-5c01595afcc5.metadata.json' at 'Branch{name=main, metadata=null, hash=ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949}'\n",
      "25/04/28 17:29:42 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=3474037751171505592, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.07028625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=3}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/28 17:29:42 INFO SparkWrite: Committed in 79 ms\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n",
      "25/04/28 17:34:06 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:34:06 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 9273cf1cd8b1:40087 in memory (size: 5.5 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create example data\n",
    "data = [\n",
    "    Row(id=1, ts=datetime(2024, 1, 10, 10, 0, 0)),\n",
    "    Row(id=2, ts=datetime(2024, 1, 11, 11, 30, 0)),\n",
    "    Row(id=3, ts=datetime(2024, 1, 12, 15, 45, 0)),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Insert into table\n",
    "df.writeTo(\"nessie.default.evolve_test\").append()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ee9d1-b1be-4885-a8ae-35dc3e5534f2",
   "metadata": {},
   "source": [
    "# alter partitioning to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6335ff04-9bc1-4cde-89f5-a09dc185a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:40:09 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}', expected commit-id was 'ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949'\n",
      "25/04/28 17:40:09 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 27 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE nessie.default.evolve_test\n",
    "ADD PARTITION FIELD months(ts)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87c8eff7-8e9b-4212-96c7-cef52cfd78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:42:02 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json\n",
      "25/04/28 17:42:02 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json' at 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}'\n",
      "25/04/28 17:42:02 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Registering RDD 67 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Got map stage job 15 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[67] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:42:02 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 16.4 KiB, free 434.2 MiB)\n",
      "25/04/28 17:42:02 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[67] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/28 17:42:02 INFO TaskSchedulerImpl: Adding task set 16.0 with 12 tasks resource profile 0\n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 59) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 60) (9273cf1cd8b1, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 61) (9273cf1cd8b1, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 62) (9273cf1cd8b1, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 63) (9273cf1cd8b1, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 64) (9273cf1cd8b1, executor driver, partition 5, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 65) (9273cf1cd8b1, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 66) (9273cf1cd8b1, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 67) (9273cf1cd8b1, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 68) (9273cf1cd8b1, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 69) (9273cf1cd8b1, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 70) (9273cf1cd8b1, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:42:02 INFO Executor: Running task 0.0 in stage 16.0 (TID 59)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 2.0 in stage 16.0 (TID 61)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 1.0 in stage 16.0 (TID 60)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 3.0 in stage 16.0 (TID 62)\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 4.0 in stage 16.0 (TID 63)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 5.0 in stage 16.0 (TID 64)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 6.0 in stage 16.0 (TID 65)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 7.0 in stage 16.0 (TID 66)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 8.0 in stage 16.0 (TID 67)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 10.0 in stage 16.0 (TID 69)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 11.0 in stage 16.0 (TID 70)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 9.0 in stage 16.0 (TID 68)\n",
      "25/04/28 17:42:02 INFO CodeGenerator: Code generated in 19.075084 ms\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 1.0 in stage 16.0 (TID 60). 2208 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 60) in 59 ms on 9273cf1cd8b1 (executor driver) (1/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 49, boot = 4, init = 45, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 9.0 in stage 16.0 (TID 68). 2208 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 68) in 62 ms on 9273cf1cd8b1 (executor driver) (2/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 47, boot = 5, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 62, boot = 17, init = 44, finish = 1\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 62, boot = 20, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 59, boot = 15, init = 44, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 10.0 in stage 16.0 (TID 69). 2294 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 65, boot = 23, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 0.0 in stage 16.0 (TID 59). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 7.0 in stage 16.0 (TID 66). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 69) in 75 ms on 9273cf1cd8b1 (executor driver) (3/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 59) in 78 ms on 9273cf1cd8b1 (executor driver) (4/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 8.0 in stage 16.0 (TID 67). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 6.0 in stage 16.0 (TID 65). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 70, boot = 27, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 66) in 82 ms on 9273cf1cd8b1 (executor driver) (5/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 65) in 83 ms on 9273cf1cd8b1 (executor driver) (6/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 67) in 83 ms on 9273cf1cd8b1 (executor driver) (7/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 4.0 in stage 16.0 (TID 63). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 56, boot = 12, init = 44, finish = 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 63) in 85 ms on 9273cf1cd8b1 (executor driver) (8/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 5.0 in stage 16.0 (TID 64). 2380 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 64) in 87 ms on 9273cf1cd8b1 (executor driver) (9/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 81, boot = 39, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 3.0 in stage 16.0 (TID 62). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 62) in 92 ms on 9273cf1cd8b1 (executor driver) (10/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 84, boot = 41, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 2.0 in stage 16.0 (TID 61). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 61) in 95 ms on 9273cf1cd8b1 (executor driver) (11/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 79, boot = 37, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 11.0 in stage 16.0 (TID 70). 2380 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 70) in 98 ms on 9273cf1cd8b1 (executor driver) (12/12)\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:42:03 INFO DAGScheduler: ShuffleMapStage 16 (append at NativeMethodAccessorImpl.java:0) finished in 0.120 s\n",
      "25/04/28 17:42:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 17:42:03 INFO DAGScheduler: running: Set()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: waiting: Set()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 17:42:03 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO SparkContext: Created broadcast 44 from broadcast at SparkWrite.java:193\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/28 17:42:03 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Got job 16 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Final stage: ResultStage 18 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Submitting ResultStage 18 (ShuffledRowRDD[68] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.4 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ShuffledRowRDD[68] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 71) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 17:42:03 INFO Executor: Running task 0.0 in stage 18.0 (TID 71)\n",
      "25/04/28 17:42:03 INFO ShuffleBlockFetcherIterator: Getting 2 (132.0 B) non-empty blocks including 2 (132.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 17:42:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 17:42:03 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 9273cf1cd8b1:40087 in memory (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:42:03 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 17:42:03 INFO DataWritingSparkTask: Committed partition 0 (task 71, attempt 0, stage 18.0)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 0.0 in stage 18.0 (TID 71). 7964 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 71) in 38 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:42:03 INFO DAGScheduler: ResultStage 18 (append at NativeMethodAccessorImpl.java:0) finished in 0.042 s\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Job 16 finished: append at NativeMethodAccessorImpl.java:0, took 0.044661 s\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 17:42:03 INFO SparkWrite: Committing append with 2 new data files to table nessie.default.evolve_test\n",
      "25/04/28 17:42:03 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}', expected commit-id was 'b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37'\n",
      "25/04/28 17:42:03 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 9 ms\n",
      "25/04/28 17:42:03 INFO SnapshotProducer: Committed snapshot 2089434004966214081 (MergeAppend)\n",
      "25/04/28 17:42:03 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:42:03 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:42:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=2089434004966214081, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.089605875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=5}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1459}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3649}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/28 17:42:03 INFO SparkWrite: Committed in 98 ms\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "data2 = [\n",
    "    Row(id=4, ts=datetime(2024, 2, 5, 9, 0, 0)),\n",
    "    Row(id=5, ts=datetime(2024, 2, 15, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"nessie.default.evolve_test\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d36f3a6-6c2a-4194-95ab-09c7c5e4fa6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|         partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "| {2024-02-05, 649}|      1|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:42:...|     2089434004966214081|\n",
      "|{2024-01-12, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "|{2024-01-11, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "|{2024-01-10, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "| {2024-02-15, 649}|      1|           1|         1|                          729|                           0|                         0|                           0|                         0|2025-04-28 17:42:...|     2089434004966214081|\n",
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:52:48 INFO V2ScanRelationPushDown: \n",
      "Output: partition#1044, spec_id#1045, record_count#1046L, file_count#1047, total_data_file_size_in_bytes#1048L, position_delete_record_count#1049L, position_delete_file_count#1050, equality_delete_record_count#1051L, equality_delete_file_count#1052, last_updated_at#1053, last_updated_snapshot_id#1054L\n",
      "         \n",
      "25/04/28 17:52:48 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 2089434004966214081 created at 2025-04-28T17:42:03.181+00:00 with filter true\n",
      "25/04/28 17:52:48 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.partitions\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 9273cf1cd8b1:40087 (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 64 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 9273cf1cd8b1:40087 (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 65 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Got job 23 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Final stage: ResultStage 25 (showString at <unknown>:0)\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[96] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 16.7 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 9273cf1cd8b1:40087 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[96] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:52:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 78) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 17039 bytes) \n",
      "25/04/28 17:52:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 78)\n",
      "25/04/28 17:52:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 78). 4496 bytes result sent to driver\n",
      "25/04/28 17:52:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 78) in 18 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:52:48 INFO DAGScheduler: ResultStage 25 (showString at <unknown>:0) finished in 0.037 s\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Job 23 finished: showString at <unknown>:0, took 0.040395 s\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.default.evolve_test.partitions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72b97803-030d-402d-bb7e-06f082d9d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 18:15:12 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 18:15:12 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 18:15:12 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 2089434004966214081 created at 2025-04-28T17:42:03.181+00:00 with filter true\n",
      "25/04/28 18:15:12 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.default.evolve_test, snapshotId=2089434004966214081, filter=true, schemaId=0, projectedFieldIds=[1, 2], projectedFieldNames=[id, ts], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.017859625S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=5}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=2}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=2}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=3649}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}, dvs=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.4, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7), app-id=local-1745785986304, engine-name=spark}}\n",
      "25/04/28 18:15:12 INFO AuthManagers: Loading AuthManager implementation: org.apache.iceberg.rest.auth.NoopAuthManager\n",
      "25/04/28 18:15:12 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n",
      "25/04/28 18:15:12 INFO V2ScanRelationPushDown: \n",
      "Output: id#1147L, ts#1148\n",
      "         \n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 67 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 68 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 5.476958 ms\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 7.785958 ms\n",
      "25/04/28 18:15:12 INFO SparkContext: Starting job: save at SparkBinPackDataRewriter.java:63\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Got job 24 (save at SparkBinPackDataRewriter.java:63) with 1 output partitions\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Final stage: ResultStage 26 (save at SparkBinPackDataRewriter.java:63)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[102] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 18.7 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 9273cf1cd8b1:40087 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[102] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 79) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 15036 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 26.0 (TID 79)\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 3.318375 ms\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 3.085125 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 26.0 (TID 79). 4752 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 79) in 53 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ResultStage 26 (save at SparkBinPackDataRewriter.java:63) finished in 0.063 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 24 finished: save at SparkBinPackDataRewriter.java:63, took 0.064933 s\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 70 from broadcast at SparkWrite.java:193\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 3 partitions.\n",
      "25/04/28 18:15:12 INFO SparkContext: Starting job: save at SparkBinPackDataRewriter.java:63\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Registering RDD 103 (save at SparkBinPackDataRewriter.java:63) as input to shuffle 2\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Got job 25 (save at SparkBinPackDataRewriter.java:63) with 3 output partitions\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Final stage: ResultStage 28 (save at SparkBinPackDataRewriter.java:63)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[103] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[103] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 80) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 15025 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 27.0 (TID 80)\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 2.548583 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 80). 4784 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 9273cf1cd8b1:40087 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 80) in 31 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ShuffleMapStage 27 (save at SparkBinPackDataRewriter.java:63) finished in 0.045 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 18:15:12 INFO DAGScheduler: running: Set()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: waiting: Set(ResultStage 28)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ResultStage 28 (ShuffledRowRDD[104] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 10.4 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 28 (ShuffledRowRDD[104] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 28.0 with 3 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 81) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 82) (9273cf1cd8b1, executor driver, partition 1, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 83) (9273cf1cd8b1, executor driver, partition 2, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 28.0 (TID 81)\n",
      "25/04/28 18:15:12 INFO Executor: Running task 1.0 in stage 28.0 (TID 82)\n",
      "25/04/28 18:15:12 INFO Executor: Running task 2.0 in stage 28.0 (TID 83)\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 2 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 1 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 2 (task 83, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 0 (task 81, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 1 (task 82, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 2.0 in stage 28.0 (TID 83). 7003 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 1.0 in stage 28.0 (TID 82). 7003 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 28.0 (TID 81). 6960 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 82) in 26 ms on 9273cf1cd8b1 (executor driver) (1/3)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 83) in 26 ms on 9273cf1cd8b1 (executor driver) (2/3)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 81) in 27 ms on 9273cf1cd8b1 (executor driver) (3/3)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ResultStage 28 (save at SparkBinPackDataRewriter.java:63) finished in 0.029 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 25 finished: save at SparkBinPackDataRewriter.java:63, took 0.078835 s\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n",
      "25/04/28 18:15:12 INFO RewriteDataFilesSparkAction: Rewrite Files Ready to be Committed - Rewriting 3 files (BIN-PACK, file group 1/1, Record(null, null) (1/1)) in nessie.default.evolve_test\n",
      "25/04/28 18:15:12 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}', expected commit-id was '7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd'\n",
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 10 ms\n",
      "25/04/28 18:15:12 INFO SnapshotProducer: Committed snapshot 7810957040280438029 (BaseRewriteFiles)\n",
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:15:12 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:15:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=7810957040280438029, sequenceNumber=3, operation=replace, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.084639792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=3}, removedDataFiles=CounterResult{unit=COUNT, value=3}, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=CounterResult{unit=COUNT, value=3}, totalRecords=CounterResult{unit=COUNT, value=5}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, removedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3649}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CALL nessie.system.rewrite_data_files(\n",
    "  table => 'nessie.default.evolve_test',\n",
    "  options => map(\n",
    "    'min-input-files', '1',          \n",
    "    'target-file-size-bytes', '5242880'  \n",
    "  )\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d3a277c-3648-4cb2-bc4f-052e9b9553c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|content|file_path                                                                                                                                                                     |file_format|spec_id|partition        |record_count|file_size_in_bytes|column_sizes      |value_counts    |null_value_counts|nan_value_counts|lower_bounds                                                    |upper_bounds                                                    |key_metadata|split_offsets|equality_ids|sort_order_id|referenced_data_file|content_offset|content_size_in_bytes|readable_metrics                                                                    |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-10/ts_month=2024-01/00000-81-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-10, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [01 00 00 00 00 00 00 00], 2 -> [00 E8 77 7E 94 0E 06 00]}|{1 -> [01 00 00 00 00 00 00 00], 2 -> [00 E8 77 7E 94 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 1, 1}, {49, 1, 0, NULL, 2024-01-10 10:00:00, 2024-01-10 10:00:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-11/ts_month=2024-01/00001-82-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-11, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [02 00 00 00 00 00 00 00], 2 -> [00 BE 2C DE A9 0E 06 00]}|{1 -> [02 00 00 00 00 00 00 00], 2 -> [00 BE 2C DE A9 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 2, 2}, {49, 1, 0, NULL, 2024-01-11 11:30:00, 2024-01-11 11:30:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-12/ts_month=2024-01/00002-83-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-12, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [03 00 00 00 00 00 00 00], 2 -> [00 97 F7 8B C1 0E 06 00]}|{1 -> [03 00 00 00 00 00 00 00], 2 -> [00 97 F7 8B C1 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 3, 3}, {49, 1, 0, NULL, 2024-01-12 15:45:00, 2024-01-12 15:45:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-02-05/ts_month=2024-02/00000-71-170fdb52-6d03-447e-8e4a-0f9aa30adfe9-0-00001.parquet|PARQUET    |1      |{2024-02-05, 649}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 04 C4 AF 9E 10 06 00]}|{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 04 C4 AF 9E 10 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 4, 4}, {49, 1, 0, NULL, 2024-02-05 09:00:00, 2024-02-05 09:00:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-02-15/ts_month=2024-02/00000-71-170fdb52-6d03-447e-8e4a-0f9aa30adfe9-0-00002.parquet|PARQUET    |1      |{2024-02-15, 649}|1           |729               |{1 -> 49, 2 -> 48}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 12 81 23 6E 11 06 00]}|{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 12 81 23 6E 11 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 5, 5}, {48, 1, 0, NULL, 2024-02-15 16:30:00, 2024-02-15 16:30:00}}|\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 18:34:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:34:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:34:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test.files\n",
      "25/04/28 18:34:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:34:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:34:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 18:34:33 INFO V2ScanRelationPushDown: \n",
      "Output: content#1465, file_path#1466, file_format#1467, spec_id#1468, partition#1469, record_count#1470L, file_size_in_bytes#1471L, column_sizes#1472, value_counts#1473, null_value_counts#1474, nan_value_counts#1475, lower_bounds#1476, upper_bounds#1477, key_metadata#1478, split_offsets#1479, equality_ids#1480, sort_order_id#1481, referenced_data_file#1482, content_offset#1483L, content_size_in_bytes#1484L, readable_metrics#1485\n",
      "         \n",
      "25/04/28 18:34:33 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 7810957040280438029 created at 2025-04-28T18:15:12.930+00:00 with filter true\n",
      "25/04/28 18:34:33 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.files\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 79 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 80 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:34:33 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Got job 28 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Final stage: ResultStage 31 (showString at <unknown>:0)\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[116] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 38.3 KiB, free 434.2 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 11.3 KiB, free 434.2 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 9273cf1cd8b1:40087 (size: 11.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[116] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:34:33 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 86) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 43029 bytes) \n",
      "25/04/28 18:34:33 INFO Executor: Running task 0.0 in stage 31.0 (TID 86)\n",
      "25/04/28 18:34:33 INFO Executor: Finished task 0.0 in stage 31.0 (TID 86). 5213 bytes result sent to driver\n",
      "25/04/28 18:34:33 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 86) in 22 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:34:33 INFO DAGScheduler: ResultStage 31 (showString at <unknown>:0) finished in 0.024 s\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Job 28 finished: showString at <unknown>:0, took 0.027048 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.default.evolve_test.files\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45b945-40c6-4b36-b2a0-87d03862078e",
   "metadata": {},
   "source": [
    "👉 Iceberg partitions are logical groups in metadata to optimize query planning and file skipping,\n",
    "👉 not strict control of how many physical folders/files you have in storage.\n",
    "\n",
    "✅ You can have 1,000 files for the same logical partition if small writes happen — and Iceberg still efficiently filters them at query time.\n",
    "\n",
    "✅ To physically consolidate small files — rewrite/compact must be triggered manually or automatically by a system like Flink Iceberg or Spark Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f669d86-051b-493d-9ae9-326c961ce544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Parsed Logical Plan ==\\n'Project [*]\\n+- 'Filter (('ts >= 'TIMESTAMP(2024-01-01)) AND ('ts <= 'TIMESTAMP(2024-01-31)))\\n   +- 'UnresolvedRelation [nessie, default, evolve_test], [], false\\n\\n== Analyzed Logical Plan ==\\nid: bigint, ts: timestamp\\nProject [id#1623L, ts#1624]\\n+- Filter ((ts#1624 >= cast(2024-01-01 as timestamp)) AND (ts#1624 <= cast(2024-01-31 as timestamp)))\\n   +- SubqueryAlias nessie.default.evolve_test\\n      +- RelationV2[id#1623L, ts#1624] nessie.default.evolve_test nessie.default.evolve_test\\n\\n== Optimized Logical Plan ==\\nFilter ((isnotnull(ts#1624) AND (ts#1624 >= 2024-01-01 00:00:00)) AND (ts#1624 <= 2024-01-31 00:00:00))\\n+- RelationV2[id#1623L, ts#1624] nessie.default.evolve_test\\n\\n== Physical Plan ==\\n*(1) Filter ((isnotnull(ts#1624) AND (ts#1624 >= 2024-01-01 00:00:00)) AND (ts#1624 <= 2024-01-31 00:00:00))\\n+- *(1) ColumnarToRow\\n   +- BatchScan nessie.default.evolve_test[id#1623L, ts#1624] nessie.default.evolve_test (branch=null) [filters=ts IS NOT NULL, ts >= 1704067200000000, ts <= 1706659200000000, groupedBy=] RuntimeFilters: []\\n|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 19:55:17 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 19:55:17 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 19:55:17 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to nessie.default.evolve_test\n",
      "Pushed Filters: ts IS NOT NULL, ts >= 1704067200000000, ts <= 1706659200000000\n",
      "Post-Scan Filters: isnotnull(ts#1624),(ts#1624 >= 2024-01-01 00:00:00),(ts#1624 <= 2024-01-31 00:00:00)\n",
      "         \n",
      "25/04/28 19:55:17 INFO V2ScanRelationPushDown: \n",
      "Output: id#1623L, ts#1624\n",
      "         \n",
      "25/04/28 19:55:17 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 7810957040280438029 created at 2025-04-28T18:15:12.930+00:00 with filter ((ts IS NOT NULL AND ts >= (16-digit-int)) AND ts <= (16-digit-int))\n",
      "25/04/28 19:55:17 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 19:55:17 INFO SparkContext: Created broadcast 82 from broadcast at SparkBatch.java:85\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 19:55:17 INFO SparkContext: Created broadcast 83 from broadcast at SparkBatch.java:85\n",
      "25/04/28 19:55:17 INFO CodeGenerator: Code generated in 5.827625 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM nessie.default.evolve_test\n",
    "WHERE ts BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-01-31')\n",
    "\"\"\"\n",
    "\n",
    "# 2. Run EXPLAIN to see the plan\n",
    "spark.sql(f\"EXPLAIN EXTENDED {query}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabb9b4-f56b-49e3-83f7-68c8f8ed55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
