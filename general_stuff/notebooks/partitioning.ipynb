{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5aa9e6-65d7-4937-a72b-40a8f70fb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 09:35:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session started\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName(\"iceberg-spark\")\n",
    "\n",
    "        # Spark + Iceberg JAR already in image, no need for jars.packages\n",
    "        .set(\"spark.jars\", \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.9.0-SNAPSHOT.jar\")\n",
    "\n",
    "        # Iceberg extensions\n",
    "        .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "        # REST catalog setup\n",
    "        .set(\"spark.sql.catalog.rest\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.rest.type\", \"rest\")\n",
    "        .set(\"spark.sql.catalog.rest.uri\", \"http://iceberg-rest:8181\")\n",
    "        .set(\"spark.sql.catalog.rest.warehouse\", \"s3://warehouse\")\n",
    "        .set(\"spark.sql.catalog.rest.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.endpoint\", \"http://minio:9000\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.path-style-access\", \"true\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
    "        .set(\"spark.sql.catalog.rest.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
    "\n",
    "        # NESSIE catalog config\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', 'http://minio:9000')\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        #MINIO CREDENTIALS\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "print(\"✅ Spark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181348ea-d56b-47bb-b453-e16b670fb9ec",
   "metadata": {},
   "source": [
    "# partitioning by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e15f31-d934-4d60-bb77-42b32556b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 13:43:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/04/29 13:43:25 INFO SharedState: Warehouse path is 'file:/opt/spark/home/iceberg/notebooks/spark-warehouse'.\n",
      "25/04/29 13:43:26 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n",
      "25/04/29 13:43:26 INFO AuthManagers: Loading AuthManager implementation: org.apache.iceberg.rest.auth.NoopAuthManager\n",
      "25/04/29 13:43:26 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "721a8d83-ffe5-4b46-8469-088a4f7a8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:08:18 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}\n",
      "25/04/28 17:08:18 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "25/04/28 17:08:18 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}', expected commit-id was 'f4d0c03304e3a63565dadc77670e9a1f1893673c86b65cfd65b001fa40f38a99'\n",
      "25/04/28 17:08:18 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 16 ms\n",
      "25/04/28 17:08:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json\n",
      "25/04/28 17:08:18 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json' at 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.default.evolve_test (\n",
    "    id BIGINT,\n",
    "    ts TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(ts))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b290460-144a-4940-b824-fc9944467a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-04-28 17:29:...|3474037751171505592|               NULL|   append|s3a://warehouse/d...|{spark.app.id -> ...|\n",
      "|2025-04-28 17:42:...|2089434004966214081|3474037751171505592|   append|s3a://warehouse/d...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:43:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:43:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:43:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test.snapshots\n",
      "25/04/28 17:43:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:43:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:43:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:43:33 INFO V2ScanRelationPushDown: \n",
      "Output: committed_at#554, snapshot_id#555L, parent_id#556L, operation#557, manifest_list#558, summary#559\n",
      "         \n",
      "25/04/28 17:43:33 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.snapshots\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 46 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 47 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 9273cf1cd8b1:40087 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Got job 17 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[72] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 15.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)\n",
      "25/04/28 17:43:33 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 9273cf1cd8b1:40087 (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:43:33 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[72] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:43:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 72) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 14932 bytes) \n",
      "25/04/28 17:43:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 72)\n",
      "25/04/28 17:43:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 72). 5017 bytes result sent to driver\n",
      "25/04/28 17:43:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 72) in 4 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:43:33 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/04/28 17:43:33 INFO DAGScheduler: Job 17 finished: showString at NativeMethodAccessorImpl.java:0, took 0.018440 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie.default.evolve_test.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d914f8ea-eb41-4ad0-abfa-12281919d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+\n",
      "|      col_name| data_type|comment|\n",
      "+--------------+----------+-------+\n",
      "|            id|    bigint|   NULL|\n",
      "|            ts| timestamp|   NULL|\n",
      "|              |          |       |\n",
      "|# Partitioning|          |       |\n",
      "|        Part 0|  days(ts)|       |\n",
      "|        Part 1|months(ts)|       |\n",
      "+--------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:40:14 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json\n",
      "25/04/28 17:40:14 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json' at 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}'\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table nessie.default.evolve_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac310ac2-b018-430a-a2a4-abb57b43fdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json\n",
      "25/04/28 17:29:42 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00000-e6c2a842-05f9-4cc9-bfe2-ffed92d09797.metadata.json' at 'Branch{name=main, metadata=null, hash=62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9}'\n",
      "25/04/28 17:29:42 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 5.348041 ms\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Registering RDD 51 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Got map stage job 11 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.1 KiB, free 434.2 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.2 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Adding task set 11.0 with 12 tasks resource profile 0\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 44) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 45) (9273cf1cd8b1, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 46) (9273cf1cd8b1, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 47) (9273cf1cd8b1, executor driver, partition 3, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 48) (9273cf1cd8b1, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 49) (9273cf1cd8b1, executor driver, partition 5, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 50) (9273cf1cd8b1, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 51) (9273cf1cd8b1, executor driver, partition 7, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 52) (9273cf1cd8b1, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 53) (9273cf1cd8b1, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 54) (9273cf1cd8b1, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 55) (9273cf1cd8b1, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 0.0 in stage 11.0 (TID 44)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 1.0 in stage 11.0 (TID 45)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 3.0 in stage 11.0 (TID 47)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 2.0 in stage 11.0 (TID 46)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 4.0 in stage 11.0 (TID 48)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 5.0 in stage 11.0 (TID 49)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 6.0 in stage 11.0 (TID 50)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 7.0 in stage 11.0 (TID 51)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 8.0 in stage 11.0 (TID 52)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 9.0 in stage 11.0 (TID 53)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 10.0 in stage 11.0 (TID 54)\n",
      "25/04/28 17:29:42 INFO Executor: Running task 11.0 in stage 11.0 (TID 55)\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 5.148791 ms\n",
      "25/04/28 17:29:42 INFO CodeGenerator: Code generated in 7.418458 ms\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 289, boot = 241, init = 48, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 243, init = 46, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 244, init = 45, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 291, boot = 248, init = 43, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 298, boot = 253, init = 45, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 298, boot = 255, init = 42, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 315, boot = 273, init = 42, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 310, boot = 266, init = 44, finish = 0\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 313, boot = 270, init = 43, finish = 0\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 1.0 in stage 11.0 (TID 45). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 8.0 in stage 11.0 (TID 52). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 10.0 in stage 11.0 (TID 54). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 6.0 in stage 11.0 (TID 50). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 0.0 in stage 11.0 (TID 44). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 9.0 in stage 11.0 (TID 53). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 2.0 in stage 11.0 (TID 46). 2294 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 50) in 383 ms on 9273cf1cd8b1 (executor driver) (1/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 45) in 383 ms on 9273cf1cd8b1 (executor driver) (2/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 52) in 382 ms on 9273cf1cd8b1 (executor driver) (3/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 44) in 386 ms on 9273cf1cd8b1 (executor driver) (4/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 46) in 384 ms on 9273cf1cd8b1 (executor driver) (5/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 54) in 383 ms on 9273cf1cd8b1 (executor driver) (6/12)\n",
      "25/04/28 17:29:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37763\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 5.0 in stage 11.0 (TID 49). 2251 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 4.0 in stage 11.0 (TID 48). 2251 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 53) in 386 ms on 9273cf1cd8b1 (executor driver) (7/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 49) in 387 ms on 9273cf1cd8b1 (executor driver) (8/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 48) in 387 ms on 9273cf1cd8b1 (executor driver) (9/12)\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 290, boot = 246, init = 43, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 305, boot = 259, init = 45, finish = 1\n",
      "25/04/28 17:29:42 INFO PythonRunner: Times: total = 295, boot = 250, init = 44, finish = 1\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 3.0 in stage 11.0 (TID 47). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 7.0 in stage 11.0 (TID 51). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 11.0 in stage 11.0 (TID 55). 2380 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 47) in 409 ms on 9273cf1cd8b1 (executor driver) (10/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 51) in 408 ms on 9273cf1cd8b1 (executor driver) (11/12)\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 55) in 409 ms on 9273cf1cd8b1 (executor driver) (12/12)\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:29:42 INFO DAGScheduler: ShuffleMapStage 11 (append at NativeMethodAccessorImpl.java:0) finished in 0.436 s\n",
      "25/04/28 17:29:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 17:29:42 INFO DAGScheduler: running: Set()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: waiting: Set()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 17:29:42 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 9273cf1cd8b1:40087 in memory (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 35 from broadcast at SparkWrite.java:193\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/28 17:29:42 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Got job 12 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Final stage: ResultStage 13 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting ResultStage 13 (ShuffledRowRDD[52] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 10.4 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/28 17:29:42 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:29:42 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (ShuffledRowRDD[52] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 56) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 17:29:42 INFO Executor: Running task 0.0 in stage 13.0 (TID 56)\n",
      "25/04/28 17:29:42 INFO ShuffleBlockFetcherIterator: Getting 3 (198.0 B) non-empty blocks including 3 (198.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 17:29:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:29:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 17:29:42 INFO DataWritingSparkTask: Committed partition 0 (task 56, attempt 0, stage 13.0)\n",
      "25/04/28 17:29:42 INFO Executor: Finished task 0.0 in stage 13.0 (TID 56). 8608 bytes result sent to driver\n",
      "25/04/28 17:29:42 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 56) in 94 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:29:42 INFO DAGScheduler: ResultStage 13 (append at NativeMethodAccessorImpl.java:0) finished in 0.109 s\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:29:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "25/04/28 17:29:42 INFO DAGScheduler: Job 12 finished: append at NativeMethodAccessorImpl.java:0, took 0.115736 s\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 17:29:42 INFO SparkWrite: Committing append with 3 new data files to table nessie.default.evolve_test\n",
      "25/04/28 17:29:42 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949}', expected commit-id was '62cb9f22976da05c9406cb22f4a3e5f78b476ff68b1009a35f4f96949a4268f9'\n",
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 11 ms\n",
      "25/04/28 17:29:42 INFO SnapshotProducer: Committed snapshot 3474037751171505592 (MergeAppend)\n",
      "25/04/28 17:29:42 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00001-fc9390f6-1bf5-47a1-a6fe-5c01595afcc5.metadata.json\n",
      "25/04/28 17:29:42 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00001-fc9390f6-1bf5-47a1-a6fe-5c01595afcc5.metadata.json' at 'Branch{name=main, metadata=null, hash=ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949}'\n",
      "25/04/28 17:29:42 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=3474037751171505592, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.07028625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=3}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/28 17:29:42 INFO SparkWrite: Committed in 79 ms\n",
      "25/04/28 17:29:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n",
      "25/04/28 17:34:06 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:34:06 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 9273cf1cd8b1:40087 in memory (size: 5.5 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create example data\n",
    "data = [\n",
    "    Row(id=1, ts=datetime(2024, 1, 10, 10, 0, 0)),\n",
    "    Row(id=2, ts=datetime(2024, 1, 11, 11, 30, 0)),\n",
    "    Row(id=3, ts=datetime(2024, 1, 12, 15, 45, 0)),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Insert into table\n",
    "df.writeTo(\"nessie.default.evolve_test\").append()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ee9d1-b1be-4885-a8ae-35dc3e5534f2",
   "metadata": {},
   "source": [
    "# alter partitioning to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6335ff04-9bc1-4cde-89f5-a09dc185a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:40:09 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}', expected commit-id was 'ead8774eb36bf8b1d8084bdd0d0d3efe71272a3101533a03bcc832e1e32dd949'\n",
      "25/04/28 17:40:09 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 27 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE nessie.default.evolve_test\n",
    "ADD PARTITION FIELD months(ts)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87c8eff7-8e9b-4212-96c7-cef52cfd78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:42:02 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json\n",
      "25/04/28 17:42:02 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00002-9cd2ab10-2aca-41d6-b28e-682e7c90fa1d.metadata.json' at 'Branch{name=main, metadata=null, hash=b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37}'\n",
      "25/04/28 17:42:02 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Registering RDD 67 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Got map stage job 15 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[67] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:42:02 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 16.4 KiB, free 434.2 MiB)\n",
      "25/04/28 17:42:02 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[67] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/28 17:42:02 INFO TaskSchedulerImpl: Adding task set 16.0 with 12 tasks resource profile 0\n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 59) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 60) (9273cf1cd8b1, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 61) (9273cf1cd8b1, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 62) (9273cf1cd8b1, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 63) (9273cf1cd8b1, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 64) (9273cf1cd8b1, executor driver, partition 5, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 65) (9273cf1cd8b1, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 66) (9273cf1cd8b1, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 67) (9273cf1cd8b1, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 68) (9273cf1cd8b1, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 69) (9273cf1cd8b1, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/28 17:42:02 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 70) (9273cf1cd8b1, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/28 17:42:02 INFO Executor: Running task 0.0 in stage 16.0 (TID 59)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 2.0 in stage 16.0 (TID 61)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 1.0 in stage 16.0 (TID 60)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 3.0 in stage 16.0 (TID 62)\n",
      "25/04/28 17:42:02 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 9273cf1cd8b1:40087 in memory (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 4.0 in stage 16.0 (TID 63)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 5.0 in stage 16.0 (TID 64)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 6.0 in stage 16.0 (TID 65)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 7.0 in stage 16.0 (TID 66)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 8.0 in stage 16.0 (TID 67)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 10.0 in stage 16.0 (TID 69)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 11.0 in stage 16.0 (TID 70)\n",
      "25/04/28 17:42:02 INFO Executor: Running task 9.0 in stage 16.0 (TID 68)\n",
      "25/04/28 17:42:02 INFO CodeGenerator: Code generated in 19.075084 ms\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 1.0 in stage 16.0 (TID 60). 2208 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 60) in 59 ms on 9273cf1cd8b1 (executor driver) (1/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 49, boot = 4, init = 45, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 9.0 in stage 16.0 (TID 68). 2208 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 68) in 62 ms on 9273cf1cd8b1 (executor driver) (2/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 47, boot = 5, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 62, boot = 17, init = 44, finish = 1\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 62, boot = 20, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 59, boot = 15, init = 44, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 10.0 in stage 16.0 (TID 69). 2294 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 65, boot = 23, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 0.0 in stage 16.0 (TID 59). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 7.0 in stage 16.0 (TID 66). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 69) in 75 ms on 9273cf1cd8b1 (executor driver) (3/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 59) in 78 ms on 9273cf1cd8b1 (executor driver) (4/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 8.0 in stage 16.0 (TID 67). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 6.0 in stage 16.0 (TID 65). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 70, boot = 27, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 66) in 82 ms on 9273cf1cd8b1 (executor driver) (5/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 65) in 83 ms on 9273cf1cd8b1 (executor driver) (6/12)\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 67) in 83 ms on 9273cf1cd8b1 (executor driver) (7/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 4.0 in stage 16.0 (TID 63). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 56, boot = 12, init = 44, finish = 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 63) in 85 ms on 9273cf1cd8b1 (executor driver) (8/12)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 5.0 in stage 16.0 (TID 64). 2380 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 64) in 87 ms on 9273cf1cd8b1 (executor driver) (9/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 81, boot = 39, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 3.0 in stage 16.0 (TID 62). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 62) in 92 ms on 9273cf1cd8b1 (executor driver) (10/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 84, boot = 41, init = 43, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 2.0 in stage 16.0 (TID 61). 2251 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 61) in 95 ms on 9273cf1cd8b1 (executor driver) (11/12)\n",
      "25/04/28 17:42:03 INFO PythonRunner: Times: total = 79, boot = 37, init = 42, finish = 0\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 11.0 in stage 16.0 (TID 70). 2380 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 70) in 98 ms on 9273cf1cd8b1 (executor driver) (12/12)\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:42:03 INFO DAGScheduler: ShuffleMapStage 16 (append at NativeMethodAccessorImpl.java:0) finished in 0.120 s\n",
      "25/04/28 17:42:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 17:42:03 INFO DAGScheduler: running: Set()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: waiting: Set()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 17:42:03 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO SparkContext: Created broadcast 44 from broadcast at SparkWrite.java:193\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/28 17:42:03 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Got job 16 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Final stage: ResultStage 18 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Submitting ResultStage 18 (ShuffledRowRDD[68] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.4 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (ShuffledRowRDD[68] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 71) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 17:42:03 INFO Executor: Running task 0.0 in stage 18.0 (TID 71)\n",
      "25/04/28 17:42:03 INFO ShuffleBlockFetcherIterator: Getting 2 (132.0 B) non-empty blocks including 2 (132.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 17:42:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 17:42:03 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:42:03 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 9273cf1cd8b1:40087 in memory (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:42:03 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 17:42:03 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 17:42:03 INFO DataWritingSparkTask: Committed partition 0 (task 71, attempt 0, stage 18.0)\n",
      "25/04/28 17:42:03 INFO Executor: Finished task 0.0 in stage 18.0 (TID 71). 7964 bytes result sent to driver\n",
      "25/04/28 17:42:03 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 71) in 38 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:42:03 INFO DAGScheduler: ResultStage 18 (append at NativeMethodAccessorImpl.java:0) finished in 0.042 s\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:42:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "25/04/28 17:42:03 INFO DAGScheduler: Job 16 finished: append at NativeMethodAccessorImpl.java:0, took 0.044661 s\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 17:42:03 INFO SparkWrite: Committing append with 2 new data files to table nessie.default.evolve_test\n",
      "25/04/28 17:42:03 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}', expected commit-id was 'b946eec63ce2f93fe6f3285c08bc0e8fcd074ccbcea80294187cacea86f5ea37'\n",
      "25/04/28 17:42:03 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 9 ms\n",
      "25/04/28 17:42:03 INFO SnapshotProducer: Committed snapshot 2089434004966214081 (MergeAppend)\n",
      "25/04/28 17:42:03 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 17:42:03 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 17:42:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=2089434004966214081, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.089605875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=5}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1459}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3649}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/28 17:42:03 INFO SparkWrite: Committed in 98 ms\n",
      "25/04/28 17:42:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "data2 = [\n",
    "    Row(id=4, ts=datetime(2024, 2, 5, 9, 0, 0)),\n",
    "    Row(id=5, ts=datetime(2024, 2, 15, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"nessie.default.evolve_test\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d36f3a6-6c2a-4194-95ab-09c7c5e4fa6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|         partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "| {2024-02-05, 649}|      1|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:42:...|     2089434004966214081|\n",
      "|{2024-01-12, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "|{2024-01-11, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "|{2024-01-10, NULL}|      0|           1|         1|                          730|                           0|                         0|                           0|                         0|2025-04-28 17:29:...|     3474037751171505592|\n",
      "| {2024-02-15, 649}|      1|           1|         1|                          729|                           0|                         0|                           0|                         0|2025-04-28 17:42:...|     2089434004966214081|\n",
      "+------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:52:48 INFO V2ScanRelationPushDown: \n",
      "Output: partition#1044, spec_id#1045, record_count#1046L, file_count#1047, total_data_file_size_in_bytes#1048L, position_delete_record_count#1049L, position_delete_file_count#1050, equality_delete_record_count#1051L, equality_delete_file_count#1052, last_updated_at#1053, last_updated_snapshot_id#1054L\n",
      "         \n",
      "25/04/28 17:52:48 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 2089434004966214081 created at 2025-04-28T17:42:03.181+00:00 with filter true\n",
      "25/04/28 17:52:48 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.partitions\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 9273cf1cd8b1:40087 (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 64 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 9273cf1cd8b1:40087 (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 65 from broadcast at SparkBatch.java:85\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Got job 23 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Final stage: ResultStage 25 (showString at <unknown>:0)\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[96] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 16.7 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.2 MiB)\n",
      "25/04/28 17:52:48 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 9273cf1cd8b1:40087 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 17:52:48 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[96] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/04/28 17:52:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 78) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 17039 bytes) \n",
      "25/04/28 17:52:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 78)\n",
      "25/04/28 17:52:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 78). 4496 bytes result sent to driver\n",
      "25/04/28 17:52:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 78) in 18 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/04/28 17:52:48 INFO DAGScheduler: ResultStage 25 (showString at <unknown>:0) finished in 0.037 s\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 17:52:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/04/28 17:52:48 INFO DAGScheduler: Job 23 finished: showString at <unknown>:0, took 0.040395 s\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 9273cf1cd8b1:40087 in memory (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:04:06 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 9273cf1cd8b1:40087 in memory (size: 4.2 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.default.evolve_test.partitions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72b97803-030d-402d-bb7e-06f082d9d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json\n",
      "25/04/28 18:15:12 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00003-53cc5ed8-4d8d-4daa-a370-17535ee32a11.metadata.json' at 'Branch{name=main, metadata=null, hash=7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd}'\n",
      "25/04/28 18:15:12 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 18:15:12 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 2089434004966214081 created at 2025-04-28T17:42:03.181+00:00 with filter true\n",
      "25/04/28 18:15:12 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.default.evolve_test, snapshotId=2089434004966214081, filter=true, schemaId=0, projectedFieldIds=[1, 2], projectedFieldNames=[id, ts], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.017859625S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=5}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=2}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=2}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=3649}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}, dvs=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.4, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7), app-id=local-1745785986304, engine-name=spark}}\n",
      "25/04/28 18:15:12 INFO AuthManagers: Loading AuthManager implementation: org.apache.iceberg.rest.auth.NoopAuthManager\n",
      "25/04/28 18:15:12 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n",
      "25/04/28 18:15:12 INFO V2ScanRelationPushDown: \n",
      "Output: id#1147L, ts#1148\n",
      "         \n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 67 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 68 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 5.476958 ms\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 7.785958 ms\n",
      "25/04/28 18:15:12 INFO SparkContext: Starting job: save at SparkBinPackDataRewriter.java:63\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Got job 24 (save at SparkBinPackDataRewriter.java:63) with 1 output partitions\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Final stage: ResultStage 26 (save at SparkBinPackDataRewriter.java:63)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[102] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 18.7 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 9273cf1cd8b1:40087 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[102] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 79) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 15036 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 26.0 (TID 79)\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 3.318375 ms\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 3.085125 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 26.0 (TID 79). 4752 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 79) in 53 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ResultStage 26 (save at SparkBinPackDataRewriter.java:63) finished in 0.063 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 24 finished: save at SparkBinPackDataRewriter.java:63, took 0.064933 s\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 70 from broadcast at SparkWrite.java:193\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET). The input RDD has 3 partitions.\n",
      "25/04/28 18:15:12 INFO SparkContext: Starting job: save at SparkBinPackDataRewriter.java:63\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Registering RDD 103 (save at SparkBinPackDataRewriter.java:63) as input to shuffle 2\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Got job 25 (save at SparkBinPackDataRewriter.java:63) with 3 output partitions\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Final stage: ResultStage 28 (save at SparkBinPackDataRewriter.java:63)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[103] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 20.6 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 9273cf1cd8b1:40087 (size: 8.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[103] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 80) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 15025 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 27.0 (TID 80)\n",
      "25/04/28 18:15:12 INFO CodeGenerator: Code generated in 2.548583 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 80). 4784 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 9273cf1cd8b1:40087 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 80) in 31 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ShuffleMapStage 27 (save at SparkBinPackDataRewriter.java:63) finished in 0.045 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/28 18:15:12 INFO DAGScheduler: running: Set()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: waiting: Set(ResultStage 28)\n",
      "25/04/28 18:15:12 INFO DAGScheduler: failed: Set()\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting ResultStage 28 (ShuffledRowRDD[104] at save at SparkBinPackDataRewriter.java:63), which has no missing parents\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 10.4 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.2 MiB)\n",
      "25/04/28 18:15:12 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:15:12 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 28 (ShuffledRowRDD[104] at save at SparkBinPackDataRewriter.java:63) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Adding task set 28.0 with 3 tasks resource profile 0\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 81) (9273cf1cd8b1, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 82) (9273cf1cd8b1, executor driver, partition 1, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 83) (9273cf1cd8b1, executor driver, partition 2, NODE_LOCAL, 9207 bytes) \n",
      "25/04/28 18:15:12 INFO Executor: Running task 0.0 in stage 28.0 (TID 81)\n",
      "25/04/28 18:15:12 INFO Executor: Running task 1.0 in stage 28.0 (TID 82)\n",
      "25/04/28 18:15:12 INFO Executor: Running task 2.0 in stage 28.0 (TID 83)\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 2 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Writer for partition 1 is committing.\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 2 (task 83, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 0 (task 81, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO DataWritingSparkTask: Committed partition 1 (task 82, attempt 0, stage 28.0)\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 2.0 in stage 28.0 (TID 83). 7003 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 1.0 in stage 28.0 (TID 82). 7003 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO Executor: Finished task 0.0 in stage 28.0 (TID 81). 6960 bytes result sent to driver\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 82) in 26 ms on 9273cf1cd8b1 (executor driver) (1/3)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 83) in 26 ms on 9273cf1cd8b1 (executor driver) (2/3)\n",
      "25/04/28 18:15:12 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 81) in 27 ms on 9273cf1cd8b1 (executor driver) (3/3)\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:15:12 INFO DAGScheduler: ResultStage 28 (save at SparkBinPackDataRewriter.java:63) finished in 0.029 s\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "25/04/28 18:15:12 INFO DAGScheduler: Job 25 finished: save at SparkBinPackDataRewriter.java:63, took 0.078835 s\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) is committing.\n",
      "25/04/28 18:15:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.evolve_test, format=PARQUET) committed.\n",
      "25/04/28 18:15:12 INFO RewriteDataFilesSparkAction: Rewrite Files Ready to be Committed - Rewriting 3 files (BIN-PACK, file group 1/1, Record(null, null) (1/1)) in nessie.default.evolve_test\n",
      "25/04/28 18:15:12 INFO NessieIcebergClient: Committed 'default.evolve_test' against 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}', expected commit-id was '7a0eec1903a64204a45c141b5ee1719b329ee6aaa685e0a8824a9e3b232567cd'\n",
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Successfully committed to table default.evolve_test in 10 ms\n",
      "25/04/28 18:15:12 INFO SnapshotProducer: Committed snapshot 7810957040280438029 (BaseRewriteFiles)\n",
      "25/04/28 18:15:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:15:12 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:15:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.evolve_test, snapshotId=7810957040280438029, sequenceNumber=3, operation=replace, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.084639792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=3}, removedDataFiles=CounterResult{unit=COUNT, value=3}, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=CounterResult{unit=COUNT, value=3}, totalRecords=CounterResult{unit=COUNT, value=5}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, removedFilesSizeInBytes=CounterResult{unit=BYTES, value=2190}, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3649}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1745785986304, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CALL nessie.system.rewrite_data_files(\n",
    "  table => 'nessie.default.evolve_test',\n",
    "  options => map(\n",
    "    'min-input-files', '1',          \n",
    "    'target-file-size-bytes', '5242880'  \n",
    "  )\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d3a277c-3648-4cb2-bc4f-052e9b9553c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|content|file_path                                                                                                                                                                     |file_format|spec_id|partition        |record_count|file_size_in_bytes|column_sizes      |value_counts    |null_value_counts|nan_value_counts|lower_bounds                                                    |upper_bounds                                                    |key_metadata|split_offsets|equality_ids|sort_order_id|referenced_data_file|content_offset|content_size_in_bytes|readable_metrics                                                                    |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-10/ts_month=2024-01/00000-81-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-10, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [01 00 00 00 00 00 00 00], 2 -> [00 E8 77 7E 94 0E 06 00]}|{1 -> [01 00 00 00 00 00 00 00], 2 -> [00 E8 77 7E 94 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 1, 1}, {49, 1, 0, NULL, 2024-01-10 10:00:00, 2024-01-10 10:00:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-11/ts_month=2024-01/00001-82-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-11, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [02 00 00 00 00 00 00 00], 2 -> [00 BE 2C DE A9 0E 06 00]}|{1 -> [02 00 00 00 00 00 00 00], 2 -> [00 BE 2C DE A9 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 2, 2}, {49, 1, 0, NULL, 2024-01-11 11:30:00, 2024-01-11 11:30:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-01-12/ts_month=2024-01/00002-83-7b91163e-ad1f-429b-91de-712b5b8344cc-0-00001.parquet|PARQUET    |1      |{2024-01-12, 648}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [03 00 00 00 00 00 00 00], 2 -> [00 97 F7 8B C1 0E 06 00]}|{1 -> [03 00 00 00 00 00 00 00], 2 -> [00 97 F7 8B C1 0E 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 3, 3}, {49, 1, 0, NULL, 2024-01-12 15:45:00, 2024-01-12 15:45:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-02-05/ts_month=2024-02/00000-71-170fdb52-6d03-447e-8e4a-0f9aa30adfe9-0-00001.parquet|PARQUET    |1      |{2024-02-05, 649}|1           |730               |{1 -> 49, 2 -> 49}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 04 C4 AF 9E 10 06 00]}|{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 04 C4 AF 9E 10 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 4, 4}, {49, 1, 0, NULL, 2024-02-05 09:00:00, 2024-02-05 09:00:00}}|\n",
      "|0      |s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/data/ts_day=2024-02-15/ts_month=2024-02/00000-71-170fdb52-6d03-447e-8e4a-0f9aa30adfe9-0-00002.parquet|PARQUET    |1      |{2024-02-15, 649}|1           |729               |{1 -> 49, 2 -> 48}|{1 -> 1, 2 -> 1}|{1 -> 0, 2 -> 0} |{}              |{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 12 81 23 6E 11 06 00]}|{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 12 81 23 6E 11 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{49, 1, 0, NULL, 5, 5}, {48, 1, 0, NULL, 2024-02-15 16:30:00, 2024-02-15 16:30:00}}|\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+-----------------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 18:34:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:34:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:34:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test.files\n",
      "25/04/28 18:34:33 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 18:34:33 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 18:34:33 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 18:34:33 INFO V2ScanRelationPushDown: \n",
      "Output: content#1465, file_path#1466, file_format#1467, spec_id#1468, partition#1469, record_count#1470L, file_size_in_bytes#1471L, column_sizes#1472, value_counts#1473, null_value_counts#1474, nan_value_counts#1475, lower_bounds#1476, upper_bounds#1477, key_metadata#1478, split_offsets#1479, equality_ids#1480, sort_order_id#1481, referenced_data_file#1482, content_offset#1483L, content_size_in_bytes#1484L, readable_metrics#1485\n",
      "         \n",
      "25/04/28 18:34:33 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 7810957040280438029 created at 2025-04-28T18:15:12.930+00:00 with filter true\n",
      "25/04/28 18:34:33 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test.files\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 79 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 9273cf1cd8b1:40087 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 80 from broadcast at SparkBatch.java:85\n",
      "25/04/28 18:34:33 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Got job 28 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Final stage: ResultStage 31 (showString at <unknown>:0)\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[116] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 38.3 KiB, free 434.2 MiB)\n",
      "25/04/28 18:34:33 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 11.3 KiB, free 434.2 MiB)\n",
      "25/04/28 18:34:33 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 9273cf1cd8b1:40087 (size: 11.3 KiB, free: 434.4 MiB)\n",
      "25/04/28 18:34:33 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[116] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "25/04/28 18:34:33 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 86) (9273cf1cd8b1, executor driver, partition 0, PROCESS_LOCAL, 43029 bytes) \n",
      "25/04/28 18:34:33 INFO Executor: Running task 0.0 in stage 31.0 (TID 86)\n",
      "25/04/28 18:34:33 INFO Executor: Finished task 0.0 in stage 31.0 (TID 86). 5213 bytes result sent to driver\n",
      "25/04/28 18:34:33 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 86) in 22 ms on 9273cf1cd8b1 (executor driver) (1/1)\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "25/04/28 18:34:33 INFO DAGScheduler: ResultStage 31 (showString at <unknown>:0) finished in 0.024 s\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/28 18:34:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "25/04/28 18:34:33 INFO DAGScheduler: Job 28 finished: showString at <unknown>:0, took 0.027048 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.default.evolve_test.files\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45b945-40c6-4b36-b2a0-87d03862078e",
   "metadata": {},
   "source": [
    "👉 Iceberg partitions are logical groups in metadata to optimize query planning and file skipping,\n",
    "👉 not strict control of how many physical folders/files you have in storage.\n",
    "\n",
    "✅ You can have 1,000 files for the same logical partition if small writes happen — and Iceberg still efficiently filters them at query time.\n",
    "\n",
    "✅ To physically consolidate small files — rewrite/compact must be triggered manually or automatically by a system like Flink Iceberg or Spark Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f669d86-051b-493d-9ae9-326c961ce544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Parsed Logical Plan ==\\n'Project [*]\\n+- 'Filter (('ts >= 'TIMESTAMP(2024-01-01)) AND ('ts <= 'TIMESTAMP(2024-01-31)))\\n   +- 'UnresolvedRelation [nessie, default, evolve_test], [], false\\n\\n== Analyzed Logical Plan ==\\nid: bigint, ts: timestamp\\nProject [id#1623L, ts#1624]\\n+- Filter ((ts#1624 >= cast(2024-01-01 as timestamp)) AND (ts#1624 <= cast(2024-01-31 as timestamp)))\\n   +- SubqueryAlias nessie.default.evolve_test\\n      +- RelationV2[id#1623L, ts#1624] nessie.default.evolve_test nessie.default.evolve_test\\n\\n== Optimized Logical Plan ==\\nFilter ((isnotnull(ts#1624) AND (ts#1624 >= 2024-01-01 00:00:00)) AND (ts#1624 <= 2024-01-31 00:00:00))\\n+- RelationV2[id#1623L, ts#1624] nessie.default.evolve_test\\n\\n== Physical Plan ==\\n*(1) Filter ((isnotnull(ts#1624) AND (ts#1624 >= 2024-01-01 00:00:00)) AND (ts#1624 <= 2024-01-31 00:00:00))\\n+- *(1) ColumnarToRow\\n   +- BatchScan nessie.default.evolve_test[id#1623L, ts#1624] nessie.default.evolve_test (branch=null) [filters=ts IS NOT NULL, ts >= 1704067200000000, ts <= 1706659200000000, groupedBy=] RuntimeFilters: []\\n|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 19:55:17 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json\n",
      "25/04/28 19:55:17 INFO NessieUtil: loadTableMetadata for 'default.evolve_test' from location 's3a://warehouse/default/evolve_test_16ea6446-4247-4065-afa6-5fd5a429f0a3/metadata/00004-38f1b844-df18-4197-9c7e-2ef64496fdf6.metadata.json' at 'Branch{name=main, metadata=null, hash=f6b014ba75cdc9ef820de0493af352ac85fbc4ec57047b39da9001bb40dd2df5}'\n",
      "25/04/28 19:55:17 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to nessie.default.evolve_test\n",
      "Pushed Filters: ts IS NOT NULL, ts >= 1704067200000000, ts <= 1706659200000000\n",
      "Post-Scan Filters: isnotnull(ts#1624),(ts#1624 >= 2024-01-01 00:00:00),(ts#1624 <= 2024-01-31 00:00:00)\n",
      "         \n",
      "25/04/28 19:55:17 INFO V2ScanRelationPushDown: \n",
      "Output: id#1623L, ts#1624\n",
      "         \n",
      "25/04/28 19:55:17 INFO SnapshotScan: Scanning table nessie.default.evolve_test snapshot 7810957040280438029 created at 2025-04-28T18:15:12.930+00:00 with filter ((ts IS NOT NULL AND ts >= (16-digit-int)) AND ts <= (16-digit-int))\n",
      "25/04/28 19:55:17 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.default.evolve_test\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 19:55:17 INFO SparkContext: Created broadcast 82 from broadcast at SparkBatch.java:85\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.3 MiB)\n",
      "25/04/28 19:55:17 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 9273cf1cd8b1:40087 (size: 3.9 KiB, free: 434.4 MiB)\n",
      "25/04/28 19:55:17 INFO SparkContext: Created broadcast 83 from broadcast at SparkBatch.java:85\n",
      "25/04/28 19:55:17 INFO CodeGenerator: Code generated in 5.827625 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM nessie.default.evolve_test\n",
    "WHERE ts BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-01-31')\n",
    "\"\"\"\n",
    "\n",
    "# 2. Run EXPLAIN to see the plan\n",
    "spark.sql(f\"EXPLAIN EXTENDED {query}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffd726-adb7-4a50-9faf-1152b5ad8ab6",
   "metadata": {},
   "source": [
    "# partition by month then evolving to day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eabbbf87-466d-47e0-a7b5-51fa6dc087d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 09:35:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/04/30 09:35:38 INFO SharedState: Warehouse path is 'file:/opt/spark/home/iceberg/notebooks/spark-warehouse'.\n",
      "25/04/30 09:35:39 INFO AuthManagers: Loading AuthManager implementation: org.apache.iceberg.rest.auth.NoopAuthManager\n",
      "25/04/30 09:35:39 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n",
      "25/04/30 09:35:40 INFO RESTSessionCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/04/30 09:35:40 INFO RESTSessionCatalog: Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE default.sales (\n",
    "    id BIGINT,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (month(created_at))\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecf7615-b405-48c5-8156-24508135d53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 09:37:19 INFO CodeGenerator: Code generated in 84.518083 ms\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Registering RDD 11 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Got map stage job 0 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 09:37:19 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[11] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.0 KiB, free 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa5472e59cd7:38613 (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[11] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 09:37:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 12 tasks resource profile 0\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 09:37:20 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "25/04/30 09:37:20 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "25/04/30 09:37:20 INFO CodeGenerator: Code generated in 17.121125 ms\n",
      "25/04/30 09:37:20 INFO CodeGenerator: Code generated in 29.509375 ms + 12) / 12]\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 339, boot = 297, init = 42, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 343, boot = 301, init = 42, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 351, boot = 307, init = 44, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 335, boot = 292, init = 43, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 351, boot = 306, init = 45, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 332, boot = 288, init = 44, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 347, boot = 303, init = 44, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 355, boot = 309, init = 46, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 332, boot = 286, init = 46, finish = 0\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 338, boot = 295, init = 43, finish = 0\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2294 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 342, boot = 299, init = 42, finish = 1\n",
      "25/04/30 09:37:20 INFO PythonRunner: Times: total = 332, boot = 284, init = 48, finish = 0\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 527 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 533 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 532 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 530 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 541 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 533 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 531 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2380 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 533 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 09:37:20 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2380 bytes result sent to driver\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 534 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 534 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 532 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 534 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 09:37:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/04/30 09:37:20 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51153\n",
      "25/04/30 09:37:20 INFO DAGScheduler: ShuffleMapStage 0 (append at NativeMethodAccessorImpl.java:0) finished in 0.630 s\n",
      "25/04/30 09:37:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 09:37:20 INFO DAGScheduler: running: Set()\n",
      "25/04/30 09:37:20 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 09:37:20 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 09:37:20 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.3 MiB)\n",
      "25/04/30 09:37:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa5472e59cd7:38613 (size: 3.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO SparkContext: Created broadcast 1 from broadcast at SparkWrite.java:193\n",
      "25/04/30 09:37:20 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.sales, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 09:37:20 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Got job 1 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Final stage: ResultStage 2 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Submitting ResultStage 2 (ShuffledRowRDD[12] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 09:37:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa5472e59cd7:38613 in memory (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/30 09:37:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa5472e59cd7:38613 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 09:37:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 09:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ShuffledRowRDD[12] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 09:37:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/04/30 09:37:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 12) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 09:37:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 12)\n",
      "25/04/30 09:37:20 INFO ShuffleBlockFetcherIterator: Getting 2 (132.0 B) non-empty blocks including 2 (132.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 09:37:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "25/04/30 09:37:21 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 09:37:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 09:37:21 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 2.0)\n",
      "25/04/30 09:37:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 12). 6858 bytes result sent to driver\n",
      "25/04/30 09:37:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 12) in 882 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 09:37:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/04/30 09:37:21 INFO DAGScheduler: ResultStage 2 (append at NativeMethodAccessorImpl.java:0) finished in 0.898 s\n",
      "25/04/30 09:37:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 09:37:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/04/30 09:37:21 INFO DAGScheduler: Job 1 finished: append at NativeMethodAccessorImpl.java:0, took 0.925769 s\n",
      "25/04/30 09:37:21 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.sales, format=PARQUET) is committing.\n",
      "25/04/30 09:37:21 INFO SparkWrite: Committing append with 1 new data files to table rest.default.sales\n",
      "25/04/30 09:37:21 INFO SnapshotProducer: Committed snapshot 1426897364490564052 (MergeAppend)\n",
      "25/04/30 09:37:21 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.sales, snapshotId=1426897364490564052, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.210071333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=2}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=770}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=770}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 09:37:21 INFO SparkWrite: Committed in 249 ms\n",
      "25/04/30 09:37:21 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.sales, format=PARQUET) committed.\n",
      "25/04/30 10:05:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa5472e59cd7:38613 in memory (size: 3.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:05:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa5472e59cd7:38613 in memory (size: 5.5 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 5, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 15, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.sales\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896913f4-f7ef-44a0-ad05-211e77ae25ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 10:15:31 INFO DAGScheduler: Registering RDD 19 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/04/30 10:15:31 INFO DAGScheduler: Got map stage job 2 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 10:15:31 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 10:15:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 10:15:31 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.0 KiB, free 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa5472e59cd7:38613 (size: 8.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 10:15:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 12 tasks resource profile 0\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 14) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 15) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 16) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 17) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 18) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 19) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 20) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 21) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 22) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 23) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 24) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 10:15:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 13)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 1.0 in stage 3.0 (TID 14)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 2.0 in stage 3.0 (TID 15)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 4.0 in stage 3.0 (TID 17)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 3.0 in stage 3.0 (TID 16)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 7.0 in stage 3.0 (TID 20)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 6.0 in stage 3.0 (TID 19)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 8.0 in stage 3.0 (TID 21)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 9.0 in stage 3.0 (TID 22)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 10.0 in stage 3.0 (TID 23)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 5.0 in stage 3.0 (TID 18)\n",
      "25/04/30 10:15:32 INFO Executor: Running task 11.0 in stage 3.0 (TID 24)\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 63, boot = 19, init = 43, finish = 1\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 67, boot = 23, init = 43, finish = 1\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 4.0 in stage 3.0 (TID 17). 2294 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 70, boot = 26, init = 44, finish = 0\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 17) in 84 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 9.0 in stage 3.0 (TID 22). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 7.0 in stage 3.0 (TID 20). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 52, boot = 9, init = 42, finish = 1\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 22) in 87 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 20) in 88 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 10.0 in stage 3.0 (TID 23). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 57, boot = 14, init = 43, finish = 0\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 3.0 in stage 3.0 (TID 16). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 54, boot = 4, init = 50, finish = 0\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 16) in 97 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 23) in 96 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 8.0 in stage 3.0 (TID 21). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 65, boot = 17, init = 46, finish = 2\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 2.0 in stage 3.0 (TID 15). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 21) in 99 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 68, boot = 20, init = 47, finish = 1\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 70, boot = 26, init = 43, finish = 1\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 76, boot = 28, init = 45, finish = 3\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 6.0 in stage 3.0 (TID 19). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 19) in 120 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 15) in 133 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 1.0 in stage 3.0 (TID 14). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 13). 2251 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 14) in 136 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 138 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 70, boot = 23, init = 47, finish = 0\n",
      "25/04/30 10:15:32 INFO PythonRunner: Times: total = 63, boot = 12, init = 45, finish = 6\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 5.0 in stage 3.0 (TID 18). 2380 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 11.0 in stage 3.0 (TID 24). 2380 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 18) in 143 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 24) in 142 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 10:15:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/04/30 10:15:32 INFO DAGScheduler: ShuffleMapStage 3 (append at NativeMethodAccessorImpl.java:0) finished in 0.158 s\n",
      "25/04/30 10:15:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 10:15:32 INFO DAGScheduler: running: Set()\n",
      "25/04/30 10:15:32 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 10:15:32 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 10:15:32 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 434.3 MiB)\n",
      "25/04/30 10:15:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa5472e59cd7:38613 (size: 3.7 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa5472e59cd7:38613 in memory (size: 8.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO SparkContext: Created broadcast 4 from broadcast at SparkWrite.java:193\n",
      "25/04/30 10:15:32 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.sales, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 10:15:32 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Got job 3 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Final stage: ResultStage 5 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Submitting ResultStage 5 (ShuffledRowRDD[20] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.4 KiB, free 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/30 10:15:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa5472e59cd7:38613 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:15:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ShuffledRowRDD[20] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 10:15:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 25) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 10:15:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 25)\n",
      "25/04/30 10:15:32 INFO ShuffleBlockFetcherIterator: Getting 2 (132.0 B) non-empty blocks including 2 (132.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 10:15:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 10:15:32 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 10:15:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 10:15:32 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 5.0)\n",
      "25/04/30 10:15:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 25). 6858 bytes result sent to driver\n",
      "25/04/30 10:15:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 25) in 42 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 10:15:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/04/30 10:15:32 INFO DAGScheduler: ResultStage 5 (append at NativeMethodAccessorImpl.java:0) finished in 0.054 s\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 10:15:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/04/30 10:15:32 INFO DAGScheduler: Job 3 finished: append at NativeMethodAccessorImpl.java:0, took 0.059657 s\n",
      "25/04/30 10:15:32 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.sales, format=PARQUET) is committing.\n",
      "25/04/30 10:15:32 INFO SparkWrite: Committing append with 1 new data files to table rest.default.sales\n",
      "25/04/30 10:15:32 INFO SnapshotProducer: Committed snapshot 4991725668785381287 (MergeAppend)\n",
      "25/04/30 10:15:32 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.sales, snapshotId=4991725668785381287, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.10106275S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=4}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=770}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1540}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 10:15:32 INFO SparkWrite: Committed in 107 ms\n",
      "25/04/30 10:15:32 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.sales, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 25, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 16, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.sales\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbd9eaf-e510-4be0-ab23-91e5cff26e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|partition        |spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+-----------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|{649, 2024-02-01}|3      |1           |1         |754                          |0                           |0                         |0                           |0                         |2025-04-30 11:27:21.985|4938301397413973209     |\n",
      "|{649, 2024-02-02}|3      |1           |1         |754                          |0                           |0                         |0                           |0                         |2025-04-30 11:27:21.985|4938301397413973209     |\n",
      "|{649, 2024-02-27}|1      |1           |1         |754                          |0                           |0                         |0                           |0                         |2025-04-30 10:46:31.781|3224308302067059425     |\n",
      "|{649, 2024-02-28}|1      |1           |1         |754                          |0                           |0                         |0                           |0                         |2025-04-30 10:46:31.781|3224308302067059425     |\n",
      "|{649, NULL}      |0      |4           |2         |1540                         |0                           |0                         |0                           |0                         |2025-04-30 10:15:32.32 |4991725668785381287     |\n",
      "+-----------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:27:33 INFO V2ScanRelationPushDown: \n",
      "Output: partition#691, spec_id#692, record_count#693L, file_count#694, total_data_file_size_in_bytes#695L, position_delete_record_count#696L, position_delete_file_count#697, equality_delete_record_count#698L, equality_delete_file_count#699, last_updated_at#700, last_updated_snapshot_id#701L\n",
      "         \n",
      "25/04/30 11:27:33 INFO SnapshotScan: Scanning table rest.default.sales snapshot 4938301397413973209 created at 2025-04-30T11:27:21.985+00:00 with filter true\n",
      "25/04/30 11:27:34 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table rest.default.sales.partitions\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 4.1 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fa5472e59cd7:38613 (size: 4.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:27:34 INFO SparkContext: Created broadcast 33 from broadcast at SparkBatch.java:85\n",
      "25/04/30 11:27:34 INFO BlockManagerInfo: Removed broadcast_31_piece0 on fa5472e59cd7:38613 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.1 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fa5472e59cd7:38613 (size: 4.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:27:34 INFO SparkContext: Created broadcast 34 from broadcast at SparkBatch.java:85\n",
      "25/04/30 11:27:34 INFO BlockManagerInfo: Removed broadcast_32_piece0 on fa5472e59cd7:38613 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:27:34 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Got job 15 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[68] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 16.7 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.3 MiB)\n",
      "25/04/30 11:27:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on fa5472e59cd7:38613 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:27:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[68] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 11:27:34 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/04/30 11:27:34 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 59) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 17144 bytes) \n",
      "25/04/30 11:27:34 INFO Executor: Running task 0.0 in stage 19.0 (TID 59)\n",
      "25/04/30 11:27:34 INFO Executor: Finished task 0.0 in stage 19.0 (TID 59). 4620 bytes result sent to driver\n",
      "25/04/30 11:27:34 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 59) in 8 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 11:27:34 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/04/30 11:27:34 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 11:27:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/04/30 11:27:34 INFO DAGScheduler: Job 15 finished: showString at NativeMethodAccessorImpl.java:0, took 0.013607 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.sales.partitions\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac2d1a25-6b7a-4d1b-ba76-45baa634c72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------+-----------+-------+---------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|content|file_path                                                                                                               |file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes      |value_counts    |null_value_counts|nan_value_counts|lower_bounds                                                    |upper_bounds                                                    |key_metadata|split_offsets|equality_ids|sort_order_id|referenced_data_file|content_offset|content_size_in_bytes|readable_metrics                                                                    |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------+-----------+-------+---------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "|0      |s3://warehouse/default/sales/data/created_at_month=2024-02/00000-25-30fffd6b-16bb-49aa-a7a4-d5fb8fa0573d-0-00001.parquet|PARQUET    |0      |{649}    |2           |770               |{1 -> 57, 2 -> 57}|{1 -> 2, 2 -> 2}|{1 -> 0, 2 -> 0} |{}              |{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 72 58 41 82 11 06 00]}|{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 84 97 04 31 12 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{57, 2, 0, NULL, 2024-02-16 16:30:00, 2024-02-25 09:00:00}, {57, 2, 0, NULL, 4, 5}}|\n",
      "|0      |s3://warehouse/default/sales/data/created_at_month=2024-02/00000-12-b9f88b6d-5021-4b54-b345-215baaa3af58-0-00001.parquet|PARQUET    |0      |{649}    |2           |770               |{1 -> 57, 2 -> 57}|{1 -> 2, 2 -> 2}|{1 -> 0, 2 -> 0} |{}              |{1 -> [04 00 00 00 00 00 00 00], 2 -> [00 04 C4 AF 9E 10 06 00]}|{1 -> [05 00 00 00 00 00 00 00], 2 -> [00 12 81 23 6E 11 06 00]}|NULL        |[4]          |NULL        |0            |NULL                |NULL          |NULL                 |{{57, 2, 0, NULL, 2024-02-05 09:00:00, 2024-02-15 16:30:00}, {57, 2, 0, NULL, 4, 5}}|\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------+-----------+-------+---------+------------+------------------+------------------+----------------+-----------------+----------------+----------------------------------------------------------------+----------------------------------------------------------------+------------+-------------+------------+-------------+--------------------+--------------+---------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 10:27:00 INFO V2ScanRelationPushDown: \n",
      "Output: content#107, file_path#108, file_format#109, spec_id#110, partition#111, record_count#112L, file_size_in_bytes#113L, column_sizes#114, value_counts#115, null_value_counts#116, nan_value_counts#117, lower_bounds#118, upper_bounds#119, key_metadata#120, split_offsets#121, equality_ids#122, sort_order_id#123, referenced_data_file#124, content_offset#125L, content_size_in_bytes#126L, readable_metrics#127\n",
      "         \n",
      "25/04/30 10:27:00 INFO SnapshotScan: Scanning table rest.default.sales snapshot 4991725668785381287 created at 2025-04-30T10:15:32.320+00:00 with filter true\n",
      "25/04/30 10:27:00 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table rest.default.sales.files\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on fa5472e59cd7:38613 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:27:00 INFO SparkContext: Created broadcast 9 from broadcast at SparkBatch.java:85\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on fa5472e59cd7:38613 (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:27:00 INFO SparkContext: Created broadcast 10 from broadcast at SparkBatch.java:85\n",
      "25/04/30 10:27:00 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fa5472e59cd7:38613 in memory (size: 4.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:27:00 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fa5472e59cd7:38613 in memory (size: 4.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:27:00 INFO CodeGenerator: Code generated in 31.293041 ms\n",
      "25/04/30 10:27:00 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 37.6 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 434.3 MiB)\n",
      "25/04/30 10:27:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa5472e59cd7:38613 (size: 11.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:27:00 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 10:27:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 10:27:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "25/04/30 10:27:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 27) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 37511 bytes) \n",
      "25/04/30 10:27:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 27)\n",
      "25/04/30 10:27:01 INFO CodeGenerator: Code generated in 27.346208 ms\n",
      "25/04/30 10:27:01 INFO Executor: Finished task 0.0 in stage 7.0 (TID 27). 4919 bytes result sent to driver\n",
      "25/04/30 10:27:01 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 27) in 67 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 10:27:01 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "25/04/30 10:27:01 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.074 s\n",
      "25/04/30 10:27:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 10:27:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "25/04/30 10:27:01 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.077464 s\n",
      "25/04/30 10:27:01 INFO CodeGenerator: Code generated in 14.374041 ms\n",
      "25/04/30 10:35:35 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fa5472e59cd7:38613 in memory (size: 5.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:35:35 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fa5472e59cd7:38613 in memory (size: 11.1 KiB, free: 434.4 MiB)\n",
      "25/04/30 10:35:35 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa5472e59cd7:38613 in memory (size: 5.1 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.sales.files\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed43eb7-5356-4ed9-9d47-470a772a19a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.sales\n",
    "ADD PARTITION FIELD days(created_at)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61b48d-e209-40af-b8ca-a0fb9394f216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 28, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 27, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.sales\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64010acb-c397-493f-88ac-02360949211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### reverting back to monthly partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df94fcb-dfa6-43ff-89d0-fdbae43bb904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE default.sales DROP PARTITION FIELD month(created_at)\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d33f13f-ec4c-403e-a861-bd04809ca73b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.sales\n",
    "ADD PARTITION FIELD month(created_at)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8144f-01e1-40b3-b76e-969984b4d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 1, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 2, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.sales\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c3fd35-e4f2-445b-9fd3-5176524f8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE default.sales DROP PARTITION FIELD days(created_at)\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4515f5d-7fb0-492b-9218-2b930b562385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 3, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 4, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.sales\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a7017-3d8e-4921-b004-85c805aead44",
   "metadata": {},
   "source": [
    "## adding partition on non partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22550381-68e6-40ae-b5fd-51b4246d0ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:41:18 INFO RESTSessionCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/04/30 11:41:18 INFO RESTSessionCatalog: Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE default.product (\n",
    "    id BIGINT,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING iceberg\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c43eb70b-29a9-40af-b9c5-d581b05cb241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:41:41 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)\n",
      "25/04/30 11:41:41 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.4 MiB)\n",
      "25/04/30 11:41:41 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on fa5472e59cd7:38613 (size: 3.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:41:41 INFO SparkContext: Created broadcast 39 from broadcast at SparkWrite.java:193\n",
      "25/04/30 11:41:41 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.product, format=PARQUET). The input RDD has 12 partitions.\n",
      "25/04/30 11:41:41 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Got job 18 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Final stage: ResultStage 23 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[82] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 11:41:41 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 14.1 KiB, free 434.4 MiB)\n",
      "25/04/30 11:41:41 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.3 MiB)\n",
      "25/04/30 11:41:41 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on fa5472e59cd7:38613 (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:41:41 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 23 (MapPartitionsRDD[82] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 11:41:41 INFO TaskSchedulerImpl: Adding task set 23.0 with 12 tasks resource profile 0\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 73) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 74) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 75) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 76) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 4.0 in stage 23.0 (TID 77) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 5.0 in stage 23.0 (TID 78) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9225 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 6.0 in stage 23.0 (TID 79) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 7.0 in stage 23.0 (TID 80) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 8.0 in stage 23.0 (TID 81) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 9.0 in stage 23.0 (TID 82) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 10.0 in stage 23.0 (TID 83) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:41:41 INFO TaskSetManager: Starting task 11.0 in stage 23.0 (TID 84) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9225 bytes) \n",
      "25/04/30 11:41:41 INFO Executor: Running task 0.0 in stage 23.0 (TID 73)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 1.0 in stage 23.0 (TID 74)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 2.0 in stage 23.0 (TID 75)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 4.0 in stage 23.0 (TID 77)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 5.0 in stage 23.0 (TID 78)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 3.0 in stage 23.0 (TID 76)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 8.0 in stage 23.0 (TID 81)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 7.0 in stage 23.0 (TID 80)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 6.0 in stage 23.0 (TID 79)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 9.0 in stage 23.0 (TID 82)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 10.0 in stage 23.0 (TID 83)\n",
      "25/04/30 11:41:41 INFO Executor: Running task 11.0 in stage 23.0 (TID 84)\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 60, boot = 6, init = 54, finish = 0\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 44, boot = 2, init = 42, finish = 0\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 46, boot = 3, init = 42, finish = 1\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 2 is committing.\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 9 is committing.\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 58, boot = 4, init = 54, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 6 is committing.\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 2 (task 75, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 0 (task 73, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 69, boot = 26, init = 43, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 9 (task 82, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 8 is committing.\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 70, boot = 15, init = 54, finish = 1\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 3 is committing.\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 2.0 in stage 23.0 (TID 75). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 0.0 in stage 23.0 (TID 73). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 9.0 in stage 23.0 (TID 82). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 75) in 93 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 73) in 94 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 9.0 in stage 23.0 (TID 82) in 92 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 73, boot = 29, init = 44, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 6 (task 79, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 1 is committing.\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 6.0 in stage 23.0 (TID 79). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 3 (task 76, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 8 (task 81, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 8.0 in stage 23.0 (TID 81). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 3.0 in stage 23.0 (TID 76). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 8.0 in stage 23.0 (TID 81) in 99 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 6.0 in stage 23.0 (TID 79) in 100 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 76) in 101 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 78, boot = 32, init = 46, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 5 is committing.\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 1 (task 74, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 1.0 in stage 23.0 (TID 74). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 85, boot = 41, init = 44, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 7 is committing.\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 74) in 107 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 85, boot = 38, init = 47, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 4 is committing.\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 4 (task 77, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 7 (task 80, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 7.0 in stage 23.0 (TID 80). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 4.0 in stage 23.0 (TID 77). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 93, boot = 47, init = 45, finish = 1\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 10 is committing.\n",
      "25/04/30 11:41:41 INFO PythonRunner: Times: total = 88, boot = 44, init = 44, finish = 0\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Writer for partition 11 is committing.\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 7.0 in stage 23.0 (TID 80) in 121 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 4.0 in stage 23.0 (TID 77) in 121 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 5 (task 78, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 5.0 in stage 23.0 (TID 78). 4544 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 10 (task 83, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 5.0 in stage 23.0 (TID 78) in 124 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 10.0 in stage 23.0 (TID 83). 1983 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 10.0 in stage 23.0 (TID 83) in 124 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 11:41:41 INFO DataWritingSparkTask: Committed partition 11 (task 84, attempt 0, stage 23.0)\n",
      "25/04/30 11:41:41 INFO Executor: Finished task 11.0 in stage 23.0 (TID 84). 4544 bytes result sent to driver\n",
      "25/04/30 11:41:41 INFO TaskSetManager: Finished task 11.0 in stage 23.0 (TID 84) in 129 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 11:41:41 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "25/04/30 11:41:41 INFO DAGScheduler: ResultStage 23 (append at NativeMethodAccessorImpl.java:0) finished in 0.134 s\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 11:41:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "25/04/30 11:41:41 INFO DAGScheduler: Job 18 finished: append at NativeMethodAccessorImpl.java:0, took 0.138536 s\n",
      "25/04/30 11:41:41 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) is committing.\n",
      "25/04/30 11:41:41 INFO SparkWrite: Committing append with 2 new data files to table rest.default.product\n",
      "25/04/30 11:41:41 INFO SnapshotProducer: Committed snapshot 2426508403812764909 (MergeAppend)\n",
      "25/04/30 11:41:41 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.product, snapshotId=2426508403812764909, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.061936416S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=2}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1508}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1508}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 11:41:41 INFO SparkWrite: Committed in 67 ms\n",
      "25/04/30 11:41:41 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 1, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 2, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.product\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8defa610-c427-40af-a3b4-e3d38c6bb161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.product\n",
    "ADD PARTITION FIELD month(created_at)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ea9d595-a23c-4a1e-a88b-95a1609e7b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:43:21 INFO DAGScheduler: Registering RDD 89 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Got map stage job 19 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[89] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 16.0 KiB, free 434.3 MiB)\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.3 MiB)\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on fa5472e59cd7:38613 (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[89] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 11:43:21 INFO TaskSchedulerImpl: Adding task set 24.0 with 12 tasks resource profile 0\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 85) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 86) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 87) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 88) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 89) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 5.0 in stage 24.0 (TID 90) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 6.0 in stage 24.0 (TID 91) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 7.0 in stage 24.0 (TID 92) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 8.0 in stage 24.0 (TID 93) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 9.0 in stage 24.0 (TID 94) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 10.0 in stage 24.0 (TID 95) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 11.0 in stage 24.0 (TID 96) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9214 bytes) \n",
      "25/04/30 11:43:21 INFO Executor: Running task 0.0 in stage 24.0 (TID 85)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 1.0 in stage 24.0 (TID 86)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 2.0 in stage 24.0 (TID 87)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 3.0 in stage 24.0 (TID 88)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 4.0 in stage 24.0 (TID 89)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 5.0 in stage 24.0 (TID 90)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 6.0 in stage 24.0 (TID 91)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 7.0 in stage 24.0 (TID 92)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 8.0 in stage 24.0 (TID 93)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 10.0 in stage 24.0 (TID 95)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 9.0 in stage 24.0 (TID 94)\n",
      "25/04/30 11:43:21 INFO Executor: Running task 11.0 in stage 24.0 (TID 96)\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Removed broadcast_39_piece0 on fa5472e59cd7:38613 in memory (size: 3.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 46, boot = 3, init = 43, finish = 0\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 47, boot = 2, init = 45, finish = 0\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 45, boot = 3, init = 42, finish = 0\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 9.0 in stage 24.0 (TID 94). 2208 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Removed broadcast_40_piece0 on fa5472e59cd7:38613 in memory (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 52, boot = 8, init = 44, finish = 0\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 3.0 in stage 24.0 (TID 88). 2208 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 10.0 in stage 24.0 (TID 95). 2165 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 9.0 in stage 24.0 (TID 94) in 66 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 2.0 in stage 24.0 (TID 87). 2251 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 88) in 67 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 50, boot = 6, init = 44, finish = 0\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 10.0 in stage 24.0 (TID 95) in 67 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 87) in 69 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 11.0 in stage 24.0 (TID 96). 2294 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 11.0 in stage 24.0 (TID 96) in 71 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 72, boot = 10, init = 62, finish = 0\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 8.0 in stage 24.0 (TID 93). 2294 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 77, boot = 32, init = 45, finish = 0\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 8.0 in stage 24.0 (TID 93) in 84 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 7.0 in stage 24.0 (TID 92). 2251 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 7.0 in stage 24.0 (TID 92) in 86 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 95, boot = 47, init = 48, finish = 0\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 88, boot = 63, init = 25, finish = 0\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 101, boot = 55, init = 46, finish = 0\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 101, boot = 50, init = 51, finish = 0\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 4.0 in stage 24.0 (TID 89). 2208 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 6.0 in stage 24.0 (TID 91). 2251 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 1.0 in stage 24.0 (TID 86). 2251 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 0.0 in stage 24.0 (TID 85). 2251 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 6.0 in stage 24.0 (TID 91) in 114 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 86) in 115 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 85) in 115 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 11:43:21 INFO PythonRunner: Times: total = 91, boot = 41, init = 50, finish = 0\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 89) in 116 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 5.0 in stage 24.0 (TID 90). 2380 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 5.0 in stage 24.0 (TID 90) in 118 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 11:43:21 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "25/04/30 11:43:21 INFO DAGScheduler: ShuffleMapStage 24 (append at NativeMethodAccessorImpl.java:0) finished in 0.130 s\n",
      "25/04/30 11:43:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 11:43:21 INFO DAGScheduler: running: Set()\n",
      "25/04/30 11:43:21 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 11:43:21 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 11:43:21 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on fa5472e59cd7:38613 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO SparkContext: Created broadcast 42 from broadcast at SparkWrite.java:193\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Removed broadcast_41_piece0 on fa5472e59cd7:38613 in memory (size: 8.4 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.product, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 11:43:21 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Got job 20 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Final stage: ResultStage 26 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Submitting ResultStage 26 (ShuffledRowRDD[90] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 10.4 KiB, free 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "25/04/30 11:43:21 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on fa5472e59cd7:38613 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:43:21 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (ShuffledRowRDD[90] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 11:43:21 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 97) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 11:43:21 INFO Executor: Running task 0.0 in stage 26.0 (TID 97)\n",
      "25/04/30 11:43:21 INFO ShuffleBlockFetcherIterator: Getting 2 (132.0 B) non-empty blocks including 2 (132.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 11:43:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 11:43:21 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:43:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 11:43:21 INFO DataWritingSparkTask: Committed partition 0 (task 97, attempt 0, stage 26.0)\n",
      "25/04/30 11:43:21 INFO Executor: Finished task 0.0 in stage 26.0 (TID 97). 6817 bytes result sent to driver\n",
      "25/04/30 11:43:21 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 97) in 26 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 11:43:21 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/04/30 11:43:21 INFO DAGScheduler: ResultStage 26 (append at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 11:43:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "25/04/30 11:43:21 INFO DAGScheduler: Job 20 finished: append at NativeMethodAccessorImpl.java:0, took 0.030276 s\n",
      "25/04/30 11:43:21 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) is committing.\n",
      "25/04/30 11:43:21 INFO SparkWrite: Committing append with 1 new data files to table rest.default.product\n",
      "25/04/30 11:43:21 INFO SnapshotProducer: Committed snapshot 1800891382160453105 (MergeAppend)\n",
      "25/04/30 11:43:21 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.product, snapshotId=1800891382160453105, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.095735375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=4}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=770}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2278}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 11:43:21 INFO SparkWrite: Committed in 99 ms\n",
      "25/04/30 11:43:21 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 1, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 2, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.product\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc02f27-1f8f-413e-8953-e1f732cf5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE default.product DROP PARTITION FIELD month(created_at)\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4f3e1c9-a3ca-43de-b201-17ae757c0a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:44:11 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 11:44:11 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)\n",
      "25/04/30 11:44:11 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on fa5472e59cd7:38613 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:44:11 INFO SparkContext: Created broadcast 44 from broadcast at SparkWrite.java:193\n",
      "25/04/30 11:44:11 INFO BlockManagerInfo: Removed broadcast_43_piece0 on fa5472e59cd7:38613 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:44:11 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.product, format=PARQUET). The input RDD has 12 partitions.\n",
      "25/04/30 11:44:11 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Got job 21 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Final stage: ResultStage 27 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[96] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 11:44:11 INFO BlockManagerInfo: Removed broadcast_42_piece0 on fa5472e59cd7:38613 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:44:11 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 14.1 KiB, free 434.4 MiB)\n",
      "25/04/30 11:44:11 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.3 MiB)\n",
      "25/04/30 11:44:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on fa5472e59cd7:38613 (size: 7.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 11:44:11 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 27 (MapPartitionsRDD[96] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 11:44:11 INFO TaskSchedulerImpl: Adding task set 27.0 with 12 tasks resource profile 0\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 98) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 99) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 100) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 3.0 in stage 27.0 (TID 101) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 4.0 in stage 27.0 (TID 102) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 5.0 in stage 27.0 (TID 103) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9225 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 6.0 in stage 27.0 (TID 104) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 7.0 in stage 27.0 (TID 105) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 8.0 in stage 27.0 (TID 106) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 9.0 in stage 27.0 (TID 107) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 10.0 in stage 27.0 (TID 108) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9187 bytes) \n",
      "25/04/30 11:44:11 INFO TaskSetManager: Starting task 11.0 in stage 27.0 (TID 109) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9225 bytes) \n",
      "25/04/30 11:44:11 INFO Executor: Running task 7.0 in stage 27.0 (TID 105)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 5.0 in stage 27.0 (TID 103)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 1.0 in stage 27.0 (TID 99)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 4.0 in stage 27.0 (TID 102)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 3.0 in stage 27.0 (TID 101)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 2.0 in stage 27.0 (TID 100)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 0.0 in stage 27.0 (TID 98)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 9.0 in stage 27.0 (TID 107)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 10.0 in stage 27.0 (TID 108)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 8.0 in stage 27.0 (TID 106)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 6.0 in stage 27.0 (TID 104)\n",
      "25/04/30 11:44:11 INFO Executor: Running task 11.0 in stage 27.0 (TID 109)\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 42, boot = -50081, init = 50123, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 3 is committing.\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 42, boot = -50076, init = 50118, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 10 is committing.\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50084, init = 50131, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 7 is committing.\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50034, init = 50081, finish = 0\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50052, init = 50099, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 9 is committing.\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50039, init = 50086, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 8 is committing.\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 3 (task 101, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 10 (task 108, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 8 (task 106, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 10.0 in stage 27.0 (TID 108). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 9 (task 107, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 3.0 in stage 27.0 (TID 101). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 9.0 in stage 27.0 (TID 107). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 50, boot = -50034, init = 50084, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 1 is committing.\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 52, boot = -50063, init = 50115, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 6 is committing.\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 48, boot = -50060, init = 50108, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 0 (task 98, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 7 (task 105, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 5 is committing.\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 8.0 in stage 27.0 (TID 106). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 7.0 in stage 27.0 (TID 105). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 0.0 in stage 27.0 (TID 98). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 10.0 in stage 27.0 (TID 108) in 63 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 1 (task 99, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50040, init = 50087, finish = 0\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 1.0 in stage 27.0 (TID 99). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 4 is committing.\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 47, boot = -50085, init = 50132, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 2 is committing.\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 6 (task 104, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 3.0 in stage 27.0 (TID 101) in 75 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 9.0 in stage 27.0 (TID 107) in 75 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 6.0 in stage 27.0 (TID 104). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 8.0 in stage 27.0 (TID 106) in 75 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 7.0 in stage 27.0 (TID 105) in 75 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 11:44:11 INFO PythonRunner: Times: total = 50, boot = 3, init = 47, finish = 0\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Writer for partition 11 is committing.\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 98) in 76 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 99) in 76 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 6.0 in stage 27.0 (TID 104) in 76 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 2 (task 100, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 2.0 in stage 27.0 (TID 100). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 4 (task 102, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 100) in 77 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 4.0 in stage 27.0 (TID 102). 1983 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 4.0 in stage 27.0 (TID 102) in 79 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 5 (task 103, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO DataWritingSparkTask: Committed partition 11 (task 109, attempt 0, stage 27.0)\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 5.0 in stage 27.0 (TID 103). 4588 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO Executor: Finished task 11.0 in stage 27.0 (TID 109). 4588 bytes result sent to driver\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 5.0 in stage 27.0 (TID 103) in 90 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 11:44:11 INFO TaskSetManager: Finished task 11.0 in stage 27.0 (TID 109) in 90 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 11:44:11 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/04/30 11:44:11 INFO DAGScheduler: ResultStage 27 (append at NativeMethodAccessorImpl.java:0) finished in 0.094 s\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 11:44:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "25/04/30 11:44:11 INFO DAGScheduler: Job 21 finished: append at NativeMethodAccessorImpl.java:0, took 0.115619 s\n",
      "25/04/30 11:44:11 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) is committing.\n",
      "25/04/30 11:44:11 INFO SparkWrite: Committing append with 2 new data files to table rest.default.product\n",
      "25/04/30 11:44:11 INFO SnapshotProducer: Committed snapshot 8719137649785198886 (MergeAppend)\n",
      "25/04/30 11:44:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.product, snapshotId=8719137649785198886, sequenceNumber=3, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.097614959S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=6}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1508}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3786}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 11:44:11 INFO SparkWrite: Committed in 102 ms\n",
      "25/04/30 11:44:11 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.product, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 1, 9, 0, 0)),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 2, 16, 30, 0)),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.product\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db280e09-915a-4039-8b36-bd3a63e5753c",
   "metadata": {},
   "source": [
    "# takeaway: for partitioned table, the first partitioning scheme at table creation is how data is arranged but then at evolution only metadata changes not the physical layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3075be21-f9f3-424c-a47c-313c5378114c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `rest`.`default`.`cust` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrop table default.cust\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `rest`.`default`.`cust` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
     ]
    }
   ],
   "source": [
    "spark.sql('drop table default.cust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a045282-4515-49c7-9ec8-ac51baaee7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 13:28:33 INFO RESTSessionCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/04/30 13:28:33 INFO RESTSessionCatalog: Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE default.cust (\n",
    "    id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    name string\n",
    ")\n",
    "USING iceberg\n",
    "partitioned by (id, month(created_at))\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efc2e97e-fc2f-4589-951d-c590bbf75340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 13:29:44 INFO CodeGenerator: Code generated in 11.796167 ms\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Registering RDD 103 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Got map stage job 22 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[103] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 16.4 KiB, free 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on fa5472e59cd7:38613 (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[103] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 13:29:44 INFO TaskSchedulerImpl: Adding task set 28.0 with 12 tasks resource profile 0\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 110) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 111) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 112) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 113) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 114) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 5.0 in stage 28.0 (TID 115) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9223 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 6.0 in stage 28.0 (TID 116) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 7.0 in stage 28.0 (TID 117) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 8.0 in stage 28.0 (TID 118) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 9.0 in stage 28.0 (TID 119) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 10.0 in stage 28.0 (TID 120) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9176 bytes) \n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 11.0 in stage 28.0 (TID 121) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9222 bytes) \n",
      "25/04/30 13:29:44 INFO Executor: Running task 0.0 in stage 28.0 (TID 110)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 1.0 in stage 28.0 (TID 111)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 2.0 in stage 28.0 (TID 112)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 5.0 in stage 28.0 (TID 115)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 4.0 in stage 28.0 (TID 114)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 3.0 in stage 28.0 (TID 113)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 7.0 in stage 28.0 (TID 117)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 8.0 in stage 28.0 (TID 118)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 6.0 in stage 28.0 (TID 116)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 10.0 in stage 28.0 (TID 120)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 9.0 in stage 28.0 (TID 119)\n",
      "25/04/30 13:29:44 INFO Executor: Running task 11.0 in stage 28.0 (TID 121)\n",
      "25/04/30 13:29:44 INFO CodeGenerator: Code generated in 5.991667 ms\n",
      "25/04/30 13:29:44 INFO CodeGenerator: Code generated in 13.657916 ms\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 51, boot = 8, init = 42, finish = 1\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 49, boot = 6, init = 43, finish = 0\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 53, boot = 10, init = 43, finish = 0\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 7.0 in stage 28.0 (TID 117). 2294 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 9.0 in stage 28.0 (TID 119). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 3.0 in stage 28.0 (TID 113). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 10.0 in stage 28.0 (TID 120). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 9.0 in stage 28.0 (TID 119) in 75 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 7.0 in stage 28.0 (TID 117) in 76 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 10.0 in stage 28.0 (TID 120) in 76 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 61, boot = 17, init = 44, finish = 0\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 113) in 79 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 59, boot = 14, init = 44, finish = 1\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 63, boot = 19, init = 44, finish = 0\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 4.0 in stage 28.0 (TID 114). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 2.0 in stage 28.0 (TID 112). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 54, boot = 12, init = 42, finish = 0\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 66, boot = 23, init = 43, finish = 0\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 0.0 in stage 28.0 (TID 110). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 46, boot = 3, init = 43, finish = 0\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 65, boot = 21, init = 44, finish = 0\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 114) in 81 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 6.0 in stage 28.0 (TID 116). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO PythonRunner: Times: total = 69, boot = 25, init = 44, finish = 0\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 1.0 in stage 28.0 (TID 111). 2251 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 8.0 in stage 28.0 (TID 118). 2294 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 110) in 100 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 112) in 99 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 11.0 in stage 28.0 (TID 121). 2380 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 6.0 in stage 28.0 (TID 116) in 98 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 8.0 in stage 28.0 (TID 118) in 99 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 111) in 100 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 11.0 in stage 28.0 (TID 121) in 98 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 5.0 in stage 28.0 (TID 115). 2380 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 5.0 in stage 28.0 (TID 115) in 100 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 13:29:44 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/04/30 13:29:44 INFO DAGScheduler: ShuffleMapStage 28 (append at NativeMethodAccessorImpl.java:0) finished in 0.134 s\n",
      "25/04/30 13:29:44 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 13:29:44 INFO DAGScheduler: running: Set()\n",
      "25/04/30 13:29:44 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 13:29:44 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 13:29:44 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.3 MiB)\n",
      "25/04/30 13:29:44 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on fa5472e59cd7:38613 (size: 3.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO SparkContext: Created broadcast 47 from broadcast at SparkWrite.java:193\n",
      "25/04/30 13:29:44 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.cust, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 13:29:44 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Got job 23 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Final stage: ResultStage 30 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Submitting ResultStage 30 (ShuffledRowRDD[104] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 10.5 KiB, free 434.3 MiB)\n",
      "25/04/30 13:29:44 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.3 MiB)\n",
      "25/04/30 13:29:44 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on fa5472e59cd7:38613 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (ShuffledRowRDD[104] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 13:29:44 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "25/04/30 13:29:44 INFO BlockManagerInfo: Removed broadcast_46_piece0 on fa5472e59cd7:38613 in memory (size: 8.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 122) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 13:29:44 INFO Executor: Running task 0.0 in stage 30.0 (TID 122)\n",
      "25/04/30 13:29:44 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 13:29:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 13:29:44 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 13:29:44 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 13:29:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 13:29:44 INFO DataWritingSparkTask: Committed partition 0 (task 122, attempt 0, stage 30.0)\n",
      "25/04/30 13:29:44 INFO Executor: Finished task 0.0 in stage 30.0 (TID 122). 8023 bytes result sent to driver\n",
      "25/04/30 13:29:44 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 122) in 68 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 13:29:44 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "25/04/30 13:29:44 INFO DAGScheduler: ResultStage 30 (append at NativeMethodAccessorImpl.java:0) finished in 0.075 s\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 13:29:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "25/04/30 13:29:44 INFO DAGScheduler: Job 23 finished: append at NativeMethodAccessorImpl.java:0, took 0.077593 s\n",
      "25/04/30 13:29:44 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.cust, format=PARQUET) is committing.\n",
      "25/04/30 13:29:44 INFO SparkWrite: Committing append with 2 new data files to table rest.default.cust\n",
      "25/04/30 13:29:44 INFO SnapshotProducer: Committed snapshot 6406526373571778165 (MergeAppend)\n",
      "25/04/30 13:29:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.cust, snapshotId=6406526373571778165, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.060065375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=2}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=2}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2047}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2047}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746005734484, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 13:29:44 INFO SparkWrite: Committed in 64 ms\n",
      "25/04/30 13:29:44 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.cust, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "data2 = [\n",
    "    Row(id=4, created_at=datetime(2024, 2, 1, 9, 0, 0), name='akshay'),\n",
    "    Row(id=5, created_at=datetime(2024, 2, 2, 16, 30, 0), name='baura'),\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.writeTo(\"default.cust\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4851d9-81a4-4aa4-b3c8-cc99fa7df8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1fb8e-0aad-4b5e-af6d-aa773483af22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9019fa0-4030-44c0-9587-fcd4fec781ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667eb5df-1ac7-4f93-9de5-691b77a0ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE default.cust (\n",
    "    id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    name string\n",
    ")\n",
    "USING iceberg\n",
    "partitioned by (id, month(created_at))\"\"\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
