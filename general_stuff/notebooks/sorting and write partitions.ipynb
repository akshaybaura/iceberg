{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f596d80e-ea6c-4e15-9b34-29c8863afa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:01:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 19:01:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session started\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName(\"iceberg-spark\")\n",
    "\n",
    "        # Spark + Iceberg JAR already in image, no need for jars.packages\n",
    "        .set(\"spark.jars\", \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.9.0-SNAPSHOT.jar\")\n",
    "\n",
    "        # Iceberg extensions\n",
    "        .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "        # REST catalog setup\n",
    "        .set(\"spark.sql.catalog.rest\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.rest.type\", \"rest\")\n",
    "        .set(\"spark.sql.catalog.rest.uri\", \"http://iceberg-rest:8181\")\n",
    "        .set(\"spark.sql.catalog.rest.warehouse\", \"s3://warehouse\")\n",
    "        .set(\"spark.sql.catalog.rest.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.endpoint\", \"http://minio:9000\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.path-style-access\", \"true\")\n",
    "        .set(\"spark.sql.catalog.rest.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
    "        .set(\"spark.sql.catalog.rest.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
    "\n",
    "        # NESSIE catalog config\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', 'http://minio:9000')\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        #MINIO CREDENTIALS\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "print(\"✅ Spark session started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073b6105-d871-49cb-85a7-09effcd20288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create namespace nessie.default\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9197275-8636-49ad-a8de-d70588102e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 17:18:51 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/flow_records_66f9e8c2-3cf8-4174-9008-087167f569b4/metadata/00002-d70d30b4-d873-49ba-8c71-fedd59a210ba.metadata.json\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o78.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3a://warehouse/default/flow_records_66f9e8c2-3cf8-4174-9008-087167f569b4/metadata/00002-d70d30b4-d873-49ba-8c71-fedd59a210ba.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:242)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:225)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:221)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:291)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n\tat org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(NessieTableOperations.java:105)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:99)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)\n\tat org.apache.spark.sql.execution.datasources.v2.DropTableExec.run(DropTableExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 183B283EFAC7C829, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:124)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:81)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:59)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:74)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:43)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:79)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.executeRequest(RetryableStage2.java:93)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:56)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5848)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:240)\n\t... 76 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop table if exists nessie.default.flow_records\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3a://warehouse/default/flow_records_66f9e8c2-3cf8-4174-9008-087167f569b4/metadata/00002-d70d30b4-d873-49ba-8c71-fedd59a210ba.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:242)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:225)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:221)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:291)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n\tat org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(NessieTableOperations.java:105)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:99)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:185)\n\tat org.apache.spark.sql.execution.datasources.v2.DropTableExec.run(DropTableExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 183B283EFAC7C829, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:124)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:81)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:59)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:74)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:43)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:79)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.executeRequest(RetryableStage2.java:93)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:56)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5848)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:240)\n\t... 76 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists nessie.default.flow_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c57407-f2f4-4b0d-abe8-6099081c3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 17:20:02 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}\n",
      "25/04/30 17:20:02 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "25/04/30 17:20:02 INFO NessieIcebergClient: Committed 'default.flow_records_without_sort' against 'Branch{name=main, metadata=null, hash=45af5afc08a900a3b70144c532200f0fecea9c8fb12d87bcb4df7b036ac6ff75}', expected commit-id was '2ce1318280bbcabb94321f62da2fbf565fdf549bcbbb017da491528bdca208ef'\n",
      "25/04/30 17:20:02 INFO BaseMetastoreTableOperations: Successfully committed to table default.flow_records_without_sort in 28 ms\n",
      "25/04/30 17:20:02 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/flow_records_without_sort_16539f19-2094-41da-b2c1-54ed2c99af17/metadata/00000-a54182f0-abda-416c-8429-a58edac8547d.metadata.json\n",
      "25/04/30 17:20:02 INFO NessieUtil: loadTableMetadata for 'default.flow_records_without_sort' from location 's3a://warehouse/default/flow_records_without_sort_16539f19-2094-41da-b2c1-54ed2c99af17/metadata/00000-a54182f0-abda-416c-8429-a58edac8547d.metadata.json' at 'Branch{name=main, metadata=null, hash=45af5afc08a900a3b70144c532200f0fecea9c8fb12d87bcb4df7b036ac6ff75}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nessie.default.flow_records_without_sort (\n",
    "  flow_id STRING,\n",
    "  network_id STRING,\n",
    "  created_at TIMESTAMP,\n",
    "  bytes_sent BIGINT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (\n",
    "  identity(network_id),\n",
    "  days(created_at),\n",
    "  bucket(4, flow_id)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f943995-1248-42d4-ab49-f3d9835ea196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|             flow_id|              string|   NULL|\n",
      "|          network_id|              string|   NULL|\n",
      "|          created_at|           timestamp|   NULL|\n",
      "|          bytes_sent|              bigint|   NULL|\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|              Part 0|          network_id|       |\n",
      "|              Part 1|    days(created_at)|       |\n",
      "|              Part 2|  bucket(4, flow_id)|       |\n",
      "|                    |                    |       |\n",
      "|  # Metadata Columns|                    |       |\n",
      "|            _spec_id|                 int|       |\n",
      "|          _partition|struct<network_id...|       |\n",
      "|               _file|              string|       |\n",
      "|                _pos|              bigint|       |\n",
      "|            _deleted|             boolean|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|nessie.default.fl...|       |\n",
      "|                Type|             MANAGED|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table extended nessie.default.flow_records\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8aeff89-a7b8-44aa-9bf4-9c46bb20d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 17:01:41 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/flow_records_66f9e8c2-3cf8-4174-9008-087167f569b4/metadata/00000-8f6125f7-4695-4c33-b6b0-236c7a4be06b.metadata.json\n",
      "25/04/30 17:01:41 INFO NessieUtil: loadTableMetadata for 'default.flow_records' from location 's3a://warehouse/default/flow_records_66f9e8c2-3cf8-4174-9008-087167f569b4/metadata/00000-8f6125f7-4695-4c33-b6b0-236c7a4be06b.metadata.json' at 'Branch{name=main, metadata=null, hash=f2be538541aae1bcd477db47247aa285b111ff78a927c0fce3d0938b96d34149}'\n",
      "25/04/30 17:01:41 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.default.flow_records\n",
      "25/04/30 17:01:41 INFO NessieIcebergClient: Committed 'default.flow_records' against 'Branch{name=main, metadata=null, hash=fb78c42c701d55befb0451add6329028b68f7679b46be21e7b99577bb952dfd0}', expected commit-id was 'f2be538541aae1bcd477db47247aa285b111ff78a927c0fce3d0938b96d34149'\n",
      "25/04/30 17:01:41 INFO BaseMetastoreTableOperations: Successfully committed to table default.flow_records in 12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE nessie.default.flow_records\n",
    "WRITE ORDERED BY (network_id, created_at)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46966a0-fb19-4ef1-9f1e-fcfcce0490c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE nessie.default.flow_records\n",
    "WRITE ORDERED BY (network_id, created_at)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28755e07-3c6a-4a3e-bf9b-a17b9e209aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 17:20:19 INFO DAGScheduler: Registering RDD 18 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Got map stage job 3 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 17.3 KiB, free 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa5472e59cd7:45023 (size: 8.9 KiB, free: 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 17:20:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 12 tasks resource profile 0\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 25) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 33026 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 26) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 33782 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 27) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 33959 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 28) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 33947 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 29) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 33961 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 30) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 33925 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 31) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 33941 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 32) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 33965 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 33) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 33951 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 34) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 33969 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 35) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 33929 bytes) \n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 36) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 34085 bytes) \n",
      "25/04/30 17:20:19 INFO Executor: Running task 1.0 in stage 4.0 (TID 26)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 25)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 2.0 in stage 4.0 (TID 27)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 3.0 in stage 4.0 (TID 28)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 4.0 in stage 4.0 (TID 29)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 5.0 in stage 4.0 (TID 30)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 6.0 in stage 4.0 (TID 31)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 7.0 in stage 4.0 (TID 32)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 8.0 in stage 4.0 (TID 33)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 9.0 in stage 4.0 (TID 34)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 10.0 in stage 4.0 (TID 35)\n",
      "25/04/30 17:20:19 INFO Executor: Running task 11.0 in stage 4.0 (TID 36)\n",
      "25/04/30 17:20:19 INFO CodeGenerator: Code generated in 13.543375 ms\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 46, boot = 3, init = 42, finish = 1\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 1.0 in stage 4.0 (TID 26). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 26) in 72 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 47, boot = 3, init = 43, finish = 1\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 4.0 in stage 4.0 (TID 29). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 44, boot = 1, init = 42, finish = 1\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 51, boot = 7, init = 42, finish = 2\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 47, boot = 4, init = 42, finish = 1\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 54, boot = 10, init = 42, finish = 2\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 6.0 in stage 4.0 (TID 31). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 29) in 84 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 31) in 85 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 25). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 5.0 in stage 4.0 (TID 30). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 11.0 in stage 4.0 (TID 36). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 60, boot = 14, init = 45, finish = 1\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 53, boot = 9, init = 43, finish = 1\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 36) in 88 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 7.0 in stage 4.0 (TID 32). 2380 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 30) in 92 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 25) in 93 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 32) in 94 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 2.0 in stage 4.0 (TID 27). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 27) in 96 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 62, boot = 15, init = 46, finish = 1\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 57, boot = 12, init = 44, finish = 1\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 62, boot = 16, init = 44, finish = 2\n",
      "25/04/30 17:20:19 INFO PythonRunner: Times: total = 66, boot = 18, init = 42, finish = 6\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 3.0 in stage 4.0 (TID 28). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 8.0 in stage 4.0 (TID 33). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 28) in 98 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 33) in 99 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 9.0 in stage 4.0 (TID 34). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 10.0 in stage 4.0 (TID 35). 2337 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 34) in 98 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 35) in 99 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 17:20:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/04/30 17:20:19 INFO DAGScheduler: ShuffleMapStage 4 (append at NativeMethodAccessorImpl.java:0) finished in 0.107 s\n",
      "25/04/30 17:20:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 17:20:19 INFO DAGScheduler: running: Set()\n",
      "25/04/30 17:20:19 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 17:20:19 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 17:20:19 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)\n",
      "25/04/30 17:20:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa5472e59cd7:45023 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO SparkContext: Created broadcast 5 from broadcast at SparkWrite.java:193\n",
      "25/04/30 17:20:19 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.default.flow_records_without_sort, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 17:20:19 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Got job 4 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Final stage: ResultStage 6 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[19] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.6 KiB, free 434.3 MiB)\n",
      "25/04/30 17:20:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.3 MiB)\n",
      "25/04/30 17:20:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa5472e59cd7:45023 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[19] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 17:20:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 37) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 17:20:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 37)\n",
      "25/04/30 17:20:19 INFO ShuffleBlockFetcherIterator: Getting 12 (180.4 KiB) non-empty blocks including 12 (180.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 17:20:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fa5472e59cd7:45023 in memory (size: 8.9 KiB, free: 434.4 MiB)\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 17:20:19 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 17:20:19 INFO DataWritingSparkTask: Committed partition 0 (task 37, attempt 0, stage 6.0)\n",
      "25/04/30 17:20:19 INFO Executor: Finished task 0.0 in stage 6.0 (TID 37). 16494 bytes result sent to driver\n",
      "25/04/30 17:20:19 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 37) in 146 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 17:20:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/04/30 17:20:19 INFO DAGScheduler: ResultStage 6 (append at NativeMethodAccessorImpl.java:0) finished in 0.149 s\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 17:20:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "25/04/30 17:20:19 INFO DAGScheduler: Job 4 finished: append at NativeMethodAccessorImpl.java:0, took 0.151237 s\n",
      "25/04/30 17:20:19 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.flow_records_without_sort, format=PARQUET) is committing.\n",
      "25/04/30 17:20:19 INFO SparkWrite: Committing append with 8 new data files to table nessie.default.flow_records_without_sort\n",
      "25/04/30 17:20:19 INFO NessieIcebergClient: Committed 'default.flow_records_without_sort' against 'Branch{name=main, metadata=null, hash=64f8734148b32c20316ea52bd129d542de0390e49cd60dab1df92579b9e2650e}', expected commit-id was '45af5afc08a900a3b70144c532200f0fecea9c8fb12d87bcb4df7b036ac6ff75'\n",
      "25/04/30 17:20:19 INFO BaseMetastoreTableOperations: Successfully committed to table default.flow_records_without_sort in 9 ms\n",
      "25/04/30 17:20:19 INFO SnapshotProducer: Committed snapshot 6458870264420467186 (MergeAppend)\n",
      "25/04/30 17:20:19 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3a://warehouse/default/flow_records_without_sort_16539f19-2094-41da-b2c1-54ed2c99af17/metadata/00001-12b71565-3490-4da0-8372-df61ca9ce8f1.metadata.json\n",
      "25/04/30 17:20:19 INFO NessieUtil: loadTableMetadata for 'default.flow_records_without_sort' from location 's3a://warehouse/default/flow_records_without_sort_16539f19-2094-41da-b2c1-54ed2c99af17/metadata/00001-12b71565-3490-4da0-8372-df61ca9ce8f1.metadata.json' at 'Branch{name=main, metadata=null, hash=64f8734148b32c20316ea52bd129d542de0390e49cd60dab1df92579b9e2650e}'\n",
      "25/04/30 17:20:19 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.default.flow_records_without_sort, snapshotId=6458870264420467186, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.062853209S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=8}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=8}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=10000}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=10000}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=106653}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=106653}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746031569301, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 17:20:19 INFO SparkWrite: Committed in 74 ms\n",
      "25/04/30 17:20:19 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.default.flow_records_without_sort, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "sample_data = [\n",
    "    (\n",
    "        f\"flow_{i}\",\n",
    "        random.choice([\"net1\", \"net2\"]),\n",
    "        datetime(2025, 4, 30, random.randint(0, 23), random.randint(0, 59)),\n",
    "        random.randint(1000, 100000)\n",
    "    )\n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sample_data, [\"flow_id\", \"network_id\", \"created_at\", \"bytes_sent\"])\n",
    "df.writeTo(\"nessie.default.flow_records_without_sort\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab853c-a676-4f4a-821a-cf0184a6073e",
   "metadata": {},
   "source": [
    "# demo write distributed by partition\n",
    "### this is beneficial if the partitions are small and almost equally distributed so that each spark task gets a partition otherwise a task may get overloaded with one partition data and the spark default write implementation is hash, so it would parallelize better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4227ce-4a3d-4dca-973b-0c7a0fd9fd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:01:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/04/30 19:01:33 INFO SharedState: Warehouse path is 'file:/opt/spark/home/iceberg/notebooks/spark-warehouse'.\n",
      "25/04/30 19:01:33 INFO AuthManagers: Loading AuthManager implementation: org.apache.iceberg.rest.auth.NoopAuthManager\n",
      "25/04/30 19:01:33 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_ALREADY_EXISTS] Cannot create schema `default` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mcreate namespace rest.default\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_ALREADY_EXISTS] Cannot create schema `default` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"create namespace rest.default\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4089f4b-1e47-4498-9923-ef95fea0bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:01:44 INFO RESTSessionCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/04/30 19:01:44 INFO RESTSessionCatalog: Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE default.flow_records (\n",
    "  flow_id STRING,\n",
    "  network_id STRING,\n",
    "  created_at TIMESTAMP,\n",
    "  bytes_sent BIGINT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (\n",
    "  identity(network_id),\n",
    "  days(created_at)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dab11a8-2cff-4229-94c4-046182c935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Skewed: net1 has 500,000 records, others have 5,000\n",
    "data = [\n",
    "    (f\"flow_net1_{i}\", \"net1\", datetime(2025, 4, 30, i % 24), random.randint(1000, 100000))\n",
    "    for i in range(500_000)\n",
    "] + [\n",
    "    (f\"flow_net{i}_{j}\", f\"net{i}\", datetime(2025, 4, 30, j % 24), random.randint(1000, 100000))\n",
    "    for i in range(2, 10) for j in range(500)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"flow_id\", \"network_id\", \"created_at\", \"bytes_sent\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc048abe-4701-49f6-82ca-ebe494cdd689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:05:25 INFO CodeGenerator: Code generated in 98.643292 ms\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Registering RDD 6 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Got map stage job 0 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:05:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.9 KiB, free 434.4 MiB)\n",
      "25/04/30 19:05:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 434.4 MiB)\n",
      "25/04/30 19:05:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa5472e59cd7:35387 (size: 8.7 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:05:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:05:25 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 19:05:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 12 tasks resource profile 0\n",
      "25/04/30 19:05:25 WARN TaskSetManager: Stage 0 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/04/30 19:05:25 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/04/30 19:05:25 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "25/04/30 19:05:25 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "25/04/30 19:05:26 INFO CodeGenerator: Code generated in 49.085292 ms\n",
      "25/04/30 19:05:26 INFO CodeGenerator: Code generated in 17.600792 ms\n",
      "25/04/30 19:05:26 INFO PythonRunner: Times: total = 340, boot = 291, init = 8, finish = 41\n",
      "25/04/30 19:05:26 INFO PythonRunner: Times: total = 311, boot = 265, init = 6, finish = 40\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 303, boot = 262, init = 4, finish = 37\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2423 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2423 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 275, boot = 251, init = 5, finish = 19\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 302, boot = 256, init = 8, finish = 38\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 1298 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2423 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 1301 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 1308 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 347, boot = 298, init = 5, finish = 44\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 1311 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 1313 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 38281\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 274, boot = 254, init = 2, finish = 18\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 1325 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1332 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 277, boot = 248, init = 8, finish = 21\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 334, boot = 281, init = 13, finish = 40\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2423 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1346 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 287, boot = 260, init = 3, finish = 24\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 1345 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 274, boot = 250, init = 6, finish = 18\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 1346 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 19:05:27 INFO PythonRunner: Times: total = 357, boot = 304, init = 11, finish = 42\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2380 bytes result sent to driver\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1370 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1359 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 19:05:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:05:27 INFO DAGScheduler: ShuffleMapStage 0 (append at NativeMethodAccessorImpl.java:0) finished in 1.464 s\n",
      "25/04/30 19:05:27 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 19:05:27 INFO DAGScheduler: running: Set()\n",
      "25/04/30 19:05:27 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 19:05:27 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 19:05:27 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 19:05:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 19:05:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:05:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa5472e59cd7:35387 (size: 3.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:05:27 INFO SparkContext: Created broadcast 1 from broadcast at SparkWrite.java:193\n",
      "25/04/30 19:05:27 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 19:05:27 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Got job 1 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Final stage: ResultStage 2 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Submitting ResultStage 2 (ShuffledRowRDD[7] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:05:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa5472e59cd7:35387 in memory (size: 8.7 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:05:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)\n",
      "25/04/30 19:05:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:05:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa5472e59cd7:35387 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:05:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:05:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ShuffledRowRDD[7] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 19:05:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/04/30 19:05:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 12) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 19:05:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 12)\n",
      "25/04/30 19:05:27 INFO ShuffleBlockFetcherIterator: Getting 12 (7.1 MiB) non-empty blocks including 12 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:05:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "25/04/30 19:05:27 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]  (0 + 1) / 1]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:05:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 19:05:28 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 2.0)\n",
      "25/04/30 19:05:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 12). 16484 bytes result sent to driver\n",
      "25/04/30 19:05:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 12) in 1361 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 19:05:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:05:28 INFO DAGScheduler: ResultStage 2 (append at NativeMethodAccessorImpl.java:0) finished in 1.373 s\n",
      "25/04/30 19:05:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 19:05:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/04/30 19:05:28 INFO DAGScheduler: Job 1 finished: append at NativeMethodAccessorImpl.java:0, took 1.390456 s\n",
      "25/04/30 19:05:28 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) is committing.\n",
      "25/04/30 19:05:28 INFO SparkWrite: Committing append with 9 new data files to table rest.default.flow_records\n",
      "25/04/30 19:05:28 INFO SnapshotProducer: Committed snapshot 157981822389736193 (MergeAppend)\n",
      "25/04/30 19:05:28 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.flow_records, snapshotId=157981822389736193, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.188224833S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=9}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=504000}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=504000}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746039683687, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 19:05:28 INFO SparkWrite: Committed in 221 ms\n",
      "25/04/30 19:05:28 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"default.flow_records\").append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f1676e-84f3-4a4d-9282-3e20780fd18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.flow_records\n",
    "WRITE DISTRIBUTED BY PARTITION\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21229403-7ec4-4cf0-a60a-16a88fc91cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:07:00 INFO DAGScheduler: Registering RDD 9 (overwritePartitions at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Got map stage job 2 (overwritePartitions at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (overwritePartitions at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at overwritePartitions at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:07:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.9 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa5472e59cd7:35387 (size: 8.7 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:07:00 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at overwritePartitions at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 19:07:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 12 tasks resource profile 0\n",
      "25/04/30 19:07:00 WARN TaskSetManager: Stage 3 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 14) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 15) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 16) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 17) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 18) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 19) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 20) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 21) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 22) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 23) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/04/30 19:07:00 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 24) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/04/30 19:07:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 13)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 1.0 in stage 3.0 (TID 14)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 2.0 in stage 3.0 (TID 15)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 3.0 in stage 3.0 (TID 16)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 4.0 in stage 3.0 (TID 17)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 6.0 in stage 3.0 (TID 19)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 5.0 in stage 3.0 (TID 18)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 8.0 in stage 3.0 (TID 21)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 9.0 in stage 3.0 (TID 22)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 7.0 in stage 3.0 (TID 20)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 10.0 in stage 3.0 (TID 23)\n",
      "25/04/30 19:07:00 INFO Executor: Running task 11.0 in stage 3.0 (TID 24)\n",
      "25/04/30 19:07:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa5472e59cd7:35387 in memory (size: 3.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa5472e59cd7:35387 in memory (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 35, boot = 2, init = 10, finish = 23\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 2.0 in stage 3.0 (TID 15). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 37, boot = 7, init = 8, finish = 22\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 39, boot = 9, init = 7, finish = 23\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 26, boot = 3, init = 5, finish = 18\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 6.0 in stage 3.0 (TID 19). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 26, boot = 3, init = 3, finish = 20\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 39, boot = 12, init = 9, finish = 18\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 13). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 15) in 324 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 19) in 318 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 49, boot = 17, init = 10, finish = 22\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 329 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 11.0 in stage 3.0 (TID 24). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 1.0 in stage 3.0 (TID 14). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 41, boot = 12, init = 7, finish = 22\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 74, boot = 32, init = 12, finish = 30\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 110, boot = 40, init = 32, finish = 38\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 10.0 in stage 3.0 (TID 23). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 24) in 319 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 8.0 in stage 3.0 (TID 21). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 23) in 322 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 14) in 332 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 21) in 324 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 3.0 in stage 3.0 (TID 16). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 9.0 in stage 3.0 (TID 22). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 16) in 334 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 71, boot = 4, init = 42, finish = 25\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 22) in 324 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 4.0 in stage 3.0 (TID 17). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 17) in 327 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 19:07:01 INFO PythonRunner: Times: total = 46, boot = 8, init = 14, finish = 24\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 7.0 in stage 3.0 (TID 20). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 20) in 328 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 5.0 in stage 3.0 (TID 18). 2380 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 18) in 331 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 19:07:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:07:01 INFO DAGScheduler: ShuffleMapStage 3 (overwritePartitions at NativeMethodAccessorImpl.java:0) finished in 0.345 s\n",
      "25/04/30 19:07:01 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 19:07:01 INFO DAGScheduler: running: Set()\n",
      "25/04/30 19:07:01 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 19:07:01 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 19:07:01 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 19:07:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa5472e59cd7:35387 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:01 INFO SparkContext: Created broadcast 4 from broadcast at SparkWrite.java:193\n",
      "25/04/30 19:07:01 INFO OverwritePartitionsDynamicExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 19:07:01 INFO SparkContext: Starting job: overwritePartitions at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Got job 3 (overwritePartitions at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Final stage: ResultStage 5 (overwritePartitions at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Submitting ResultStage 5 (ShuffledRowRDD[10] at overwritePartitions at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:07:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:07:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa5472e59cd7:35387 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ShuffledRowRDD[10] at overwritePartitions at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 19:07:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 25) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 19:07:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 25)\n",
      "25/04/30 19:07:01 INFO ShuffleBlockFetcherIterator: Getting 12 (7.1 MiB) non-empty blocks including 12 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:07:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa5472e59cd7:35387 in memory (size: 8.7 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:07:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 19:07:01 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 5.0)\n",
      "25/04/30 19:07:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 25). 16441 bytes result sent to driver\n",
      "25/04/30 19:07:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 25) in 362 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 19:07:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:07:01 INFO DAGScheduler: ResultStage 5 (overwritePartitions at NativeMethodAccessorImpl.java:0) finished in 0.365 s\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 19:07:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/04/30 19:07:01 INFO DAGScheduler: Job 3 finished: overwritePartitions at NativeMethodAccessorImpl.java:0, took 0.368119 s\n",
      "25/04/30 19:07:01 INFO OverwritePartitionsDynamicExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) is committing.\n",
      "25/04/30 19:07:01 INFO SparkWrite: Committing dynamic partition overwrite with 9 new data files to table rest.default.flow_records\n",
      "25/04/30 19:07:01 INFO SnapshotProducer: Committed snapshot 5660463466131375950 (BaseReplacePartitions)\n",
      "25/04/30 19:07:01 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.flow_records, snapshotId=5660463466131375950, sequenceNumber=2, operation=overwrite, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.105165292S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=9}, removedDataFiles=CounterResult{unit=COUNT, value=9}, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=504000}, removedRecords=CounterResult{unit=COUNT, value=504000}, totalRecords=CounterResult{unit=COUNT, value=504000}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, removedFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746039683687, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 19:07:01 INFO SparkWrite: Committed in 112 ms\n",
      "25/04/30 19:07:01 INFO OverwritePartitionsDynamicExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"default.flow_records\").overwritePartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58779ed-837b-409c-b170-697213f10fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(8, \"network_id\", expr(\"date(created_at)\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61459fca-e864-412f-b5a8-b9051b19bb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:15:54 INFO DAGScheduler: Registering RDD 29 (overwritePartitions at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Got map stage job 9 (overwritePartitions at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (overwritePartitions at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[29] at overwritePartitions at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 24.2 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on fa5472e59cd7:35387 (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[29] at overwritePartitions at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Adding task set 14.0 with 12 tasks resource profile 0\n",
      "25/04/30 19:15:54 WARN TaskSetManager: Stage 14 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 64) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 65) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 66) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 67) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 68) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 69) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 70) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 71) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 72) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 73) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 74) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 75) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/04/30 19:15:54 INFO Executor: Running task 1.0 in stage 14.0 (TID 65)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 3.0 in stage 14.0 (TID 67)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 2.0 in stage 14.0 (TID 66)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 6.0 in stage 14.0 (TID 70)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 5.0 in stage 14.0 (TID 69)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 8.0 in stage 14.0 (TID 72)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 0.0 in stage 14.0 (TID 64)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 4.0 in stage 14.0 (TID 68)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 7.0 in stage 14.0 (TID 71)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 9.0 in stage 14.0 (TID 73)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 10.0 in stage 14.0 (TID 74)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 11.0 in stage 14.0 (TID 75)\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fa5472e59cd7:35387 in memory (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 32, boot = -13803, init = 13809, finish = 26\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 8.0 in stage 14.0 (TID 72). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 72) in 159 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 55, boot = -13799, init = 13830, finish = 24\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 26, boot = -13846, init = 13847, finish = 25\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 4.0 in stage 14.0 (TID 68). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 34, boot = -13806, init = 13814, finish = 26\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 2.0 in stage 14.0 (TID 66). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 68) in 167 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 1.0 in stage 14.0 (TID 65). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 65) in 171 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 66) in 171 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 27, boot = -13819, init = 13820, finish = 26\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 43, boot = -13840, init = 13855, finish = 28\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 7.0 in stage 14.0 (TID 71). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 0.0 in stage 14.0 (TID 64). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 71) in 170 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 34, boot = -13828, init = 13835, finish = 27\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 64) in 177 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 5.0 in stage 14.0 (TID 69). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 58, boot = -13844, init = 13863, finish = 39\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 69) in 172 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 33, boot = -13789, init = 13791, finish = 31\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 11.0 in stage 14.0 (TID 75). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 9.0 in stage 14.0 (TID 73). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 75) in 173 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 40, boot = -13784, init = 13791, finish = 33\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 73) in 174 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 49, boot = -13782, init = 13791, finish = 40\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 10.0 in stage 14.0 (TID 74). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 6.0 in stage 14.0 (TID 70). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 74) in 176 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 70) in 177 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 19:15:54 INFO PythonRunner: Times: total = 36, boot = -13820, init = 13822, finish = 34\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 3.0 in stage 14.0 (TID 67). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 67) in 184 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:15:54 INFO DAGScheduler: ShuffleMapStage 14 (overwritePartitions at NativeMethodAccessorImpl.java:0) finished in 0.188 s\n",
      "25/04/30 19:15:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 19:15:54 INFO DAGScheduler: running: Set()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Registering RDD 31 (overwritePartitions at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Got map stage job 10 (overwritePartitions at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (overwritePartitions at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[31] at overwritePartitions at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.3 KiB, free 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa5472e59cd7:35387 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[31] at overwritePartitions at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Adding task set 16.0 with 8 tasks resource profile 0\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 76) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 77) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 78) (fa5472e59cd7, executor driver, partition 7, NODE_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 79) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 80) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 81) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 82) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 83) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9196 bytes) \n",
      "25/04/30 19:15:54 INFO Executor: Running task 3.0 in stage 16.0 (TID 76)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 7.0 in stage 16.0 (TID 78)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 5.0 in stage 16.0 (TID 77)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 0.0 in stage 16.0 (TID 79)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 4.0 in stage 16.0 (TID 82)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 1.0 in stage 16.0 (TID 80)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 2.0 in stage 16.0 (TID 81)\n",
      "25/04/30 19:15:54 INFO Executor: Running task 6.0 in stage 16.0 (TID 83)\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 12 (7.1 MiB) non-empty blocks including 12 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 1 (14.8 KiB) non-empty blocks including 1 (14.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 1 (6.9 KiB) non-empty blocks including 1 (6.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 6.0 in stage 16.0 (TID 83). 3978 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 1.0 in stage 16.0 (TID 80). 3978 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 2.0 in stage 16.0 (TID 81). 3978 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 0.0 in stage 16.0 (TID 79). 3978 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 4.0 in stage 16.0 (TID 82). 3978 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 83) in 8 ms on fa5472e59cd7 (executor driver) (1/8)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 80) in 8 ms on fa5472e59cd7 (executor driver) (2/8)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 82) in 8 ms on fa5472e59cd7 (executor driver) (3/8)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 79) in 8 ms on fa5472e59cd7 (executor driver) (4/8)\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 81) in 8 ms on fa5472e59cd7 (executor driver) (5/8)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 3.0 in stage 16.0 (TID 76). 4107 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 76) in 11 ms on fa5472e59cd7 (executor driver) (6/8)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 7.0 in stage 16.0 (TID 78). 4107 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 78) in 11 ms on fa5472e59cd7 (executor driver) (7/8)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 5.0 in stage 16.0 (TID 77). 4150 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 77) in 112 ms on fa5472e59cd7 (executor driver) (8/8)\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:15:54 INFO DAGScheduler: ShuffleMapStage 16 (overwritePartitions at NativeMethodAccessorImpl.java:0) finished in 0.114 s\n",
      "25/04/30 19:15:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 19:15:54 INFO DAGScheduler: running: Set()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: failed: Set()\n",
      "25/04/30 19:15:54 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa5472e59cd7:35387 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO SparkContext: Created broadcast 14 from broadcast at SparkWrite.java:193\n",
      "25/04/30 19:15:54 INFO OverwritePartitionsDynamicExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/04/30 19:15:54 INFO SparkContext: Starting job: overwritePartitions at NativeMethodAccessorImpl.java:0\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Got job 11 (overwritePartitions at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Final stage: ResultStage 19 (overwritePartitions at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting ResultStage 19 (ShuffledRowRDD[32] at overwritePartitions at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 10.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa5472e59cd7:35387 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRowRDD[32] at overwritePartitions at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 84) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/04/30 19:15:54 INFO Executor: Running task 0.0 in stage 19.0 (TID 84)\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Getting 3 (7.1 MiB) non-empty blocks including 3 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/30 19:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fa5472e59cd7:35387 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/04/30 19:15:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/04/30 19:15:54 INFO DataWritingSparkTask: Committed partition 0 (task 84, attempt 0, stage 19.0)\n",
      "25/04/30 19:15:54 INFO Executor: Finished task 0.0 in stage 19.0 (TID 84). 16398 bytes result sent to driver\n",
      "25/04/30 19:15:54 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 84) in 300 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:15:54 INFO DAGScheduler: ResultStage 19 (overwritePartitions at NativeMethodAccessorImpl.java:0) finished in 0.302 s\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/30 19:15:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/04/30 19:15:54 INFO DAGScheduler: Job 11 finished: overwritePartitions at NativeMethodAccessorImpl.java:0, took 0.303982 s\n",
      "25/04/30 19:15:54 INFO OverwritePartitionsDynamicExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) is committing.\n",
      "25/04/30 19:15:54 INFO SparkWrite: Committing dynamic partition overwrite with 9 new data files to table rest.default.flow_records\n",
      "25/04/30 19:15:55 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fa5472e59cd7:35387 in memory (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:55 INFO SnapshotProducer: Committed snapshot 5492783014125842204 (BaseReplacePartitions)\n",
      "25/04/30 19:15:55 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.flow_records, snapshotId=5492783014125842204, sequenceNumber=4, operation=overwrite, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.090300333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=9}, removedDataFiles=CounterResult{unit=COUNT, value=18}, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=504000}, removedRecords=CounterResult{unit=COUNT, value=1008000}, totalRecords=CounterResult{unit=COUNT, value=504000}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, removedFilesSizeInBytes=CounterResult{unit=BYTES, value=3281242}, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1640621}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746039683687, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/04/30 19:15:55 INFO SparkWrite: Committed in 96 ms\n",
      "25/04/30 19:15:55 INFO OverwritePartitionsDynamicExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"default.flow_records\").overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abdfd8ff-28f9-4cca-b0d1-b44a5899525c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 19:15:40 INFO DAGScheduler: Registering RDD 23 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Got map stage job 8 (javaToPython at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[23] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/30 19:15:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 24.2 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "25/04/30 19:15:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on fa5472e59cd7:35387 (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/30 19:15:40 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[23] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/04/30 19:15:40 INFO TaskSchedulerImpl: Adding task set 13.0 with 12 tasks resource profile 0\n",
      "25/04/30 19:15:40 WARN TaskSetManager: Stage 13 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 52) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 53) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 54) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 55) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 56) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 57) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 58) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 59) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 60) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 61) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 62) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/04/30 19:15:40 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 63) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/04/30 19:15:40 INFO Executor: Running task 0.0 in stage 13.0 (TID 52)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 1.0 in stage 13.0 (TID 53)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 3.0 in stage 13.0 (TID 55)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 2.0 in stage 13.0 (TID 54)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 4.0 in stage 13.0 (TID 56)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 6.0 in stage 13.0 (TID 58)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 5.0 in stage 13.0 (TID 57)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 7.0 in stage 13.0 (TID 59)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 8.0 in stage 13.0 (TID 60)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 9.0 in stage 13.0 (TID 61)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 10.0 in stage 13.0 (TID 62)\n",
      "25/04/30 19:15:40 INFO Executor: Running task 11.0 in stage 13.0 (TID 63)\n",
      "25/04/30 19:15:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa5472e59cd7:35387 in memory (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/04/30 19:15:40 INFO CodeGenerator: Code generated in 59.352917 ms\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 78, boot = 56, init = 2, finish = 20\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 3.0 in stage 13.0 (TID 55). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 55) in 188 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 25, boot = 4, init = 3, finish = 18\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 6.0 in stage 13.0 (TID 58). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 58) in 186 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 49, boot = 4, init = 3, finish = 42\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 46, boot = 2, init = 3, finish = 41\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 4.0 in stage 13.0 (TID 56). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 56) in 201 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 72, boot = 44, init = 7, finish = 21\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 11.0 in stage 13.0 (TID 63). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 63) in 191 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 107, boot = 69, init = 19, finish = 19\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 89, boot = 61, init = 4, finish = 24\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 2.0 in stage 13.0 (TID 54). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 104, boot = 84, init = 3, finish = 17\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 5.0 in stage 13.0 (TID 57). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 54) in 208 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 10.0 in stage 13.0 (TID 62). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 0.0 in stage 13.0 (TID 52). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 64, boot = 10, init = 32, finish = 22\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 62) in 198 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 52) in 211 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 1.0 in stage 13.0 (TID 53). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 92, boot = 59, init = 14, finish = 19\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 57) in 206 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 53) in 215 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 74, boot = 50, init = 5, finish = 19\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 8.0 in stage 13.0 (TID 60). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO PythonRunner: Times: total = 108, boot = 88, init = 3, finish = 17\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 60) in 204 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 7.0 in stage 13.0 (TID 59). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 59) in 206 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/04/30 19:15:40 INFO Executor: Finished task 9.0 in stage 13.0 (TID 61). 2185 bytes result sent to driver\n",
      "25/04/30 19:15:40 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 61) in 205 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/04/30 19:15:40 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/04/30 19:15:40 INFO DAGScheduler: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.224 s\n",
      "25/04/30 19:15:40 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/30 19:15:40 INFO DAGScheduler: running: Set()\n",
      "25/04/30 19:15:40 INFO DAGScheduler: waiting: Set()\n",
      "25/04/30 19:15:40 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11e06450-13a2-4fcc-b406-a611aae726fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 06:32:48 INFO DAGScheduler: Registering RDD 48 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Got map stage job 17 (append at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[48] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 24.2 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fa5472e59cd7:35387 (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[48] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Adding task set 35.0 with 12 tasks resource profile 0\n",
      "25/05/01 06:32:48 WARN TaskSetManager: Stage 35 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 118) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 119) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 120) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 121) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 4.0 in stage 35.0 (TID 122) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 5.0 in stage 35.0 (TID 123) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 6.0 in stage 35.0 (TID 124) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 7.0 in stage 35.0 (TID 125) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 8.0 in stage 35.0 (TID 126) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 9.0 in stage 35.0 (TID 127) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 10.0 in stage 35.0 (TID 128) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 11.0 in stage 35.0 (TID 129) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/05/01 06:32:48 INFO Executor: Running task 2.0 in stage 35.0 (TID 120)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 1.0 in stage 35.0 (TID 119)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 0.0 in stage 35.0 (TID 118)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 5.0 in stage 35.0 (TID 123)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 3.0 in stage 35.0 (TID 121)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 4.0 in stage 35.0 (TID 122)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 7.0 in stage 35.0 (TID 125)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 6.0 in stage 35.0 (TID 124)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 8.0 in stage 35.0 (TID 126)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 9.0 in stage 35.0 (TID 127)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 11.0 in stage 35.0 (TID 129)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 10.0 in stage 35.0 (TID 128)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 34, boot = 3, init = 3, finish = 28\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fa5472e59cd7:35387 in memory (size: 11.0 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 4.0 in stage 35.0 (TID 122). 2228 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 4.0 in stage 35.0 (TID 122) in 127 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 36, boot = 8, init = 9, finish = 19\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 36, boot = 8, init = 6, finish = 22\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 2.0 in stage 35.0 (TID 120). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 0.0 in stage 35.0 (TID 118). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fa5472e59cd7:35387 in memory (size: 11.0 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 36, boot = 7, init = 6, finish = 23\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 120) in 151 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 118) in 152 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 7.0 in stage 35.0 (TID 125). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 7.0 in stage 35.0 (TID 125) in 140 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 37, boot = 6, init = 8, finish = 23\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 22, boot = 2, init = 2, finish = 18\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 63, boot = 20, init = 20, finish = 23\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 3.0 in stage 35.0 (TID 121). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 10.0 in stage 35.0 (TID 128). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 121) in 159 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 10.0 in stage 35.0 (TID 128) in 143 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 5.0 in stage 35.0 (TID 123). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 65, boot = 38, init = 7, finish = 20\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 5.0 in stage 35.0 (TID 123) in 160 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 9.0 in stage 35.0 (TID 127). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 9.0 in stage 35.0 (TID 127) in 149 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 58, boot = 14, init = 26, finish = 18\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 1.0 in stage 35.0 (TID 119). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 119) in 168 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 77, boot = 22, init = 21, finish = 34\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 8.0 in stage 35.0 (TID 126). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 8.0 in stage 35.0 (TID 126) in 155 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 33, boot = 10, init = 4, finish = 19\n",
      "25/05/01 06:32:48 INFO PythonRunner: Times: total = 66, boot = 21, init = 15, finish = 30\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 6.0 in stage 35.0 (TID 124). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 6.0 in stage 35.0 (TID 124) in 160 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 11.0 in stage 35.0 (TID 129). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 11.0 in stage 35.0 (TID 129) in 157 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:48 INFO DAGScheduler: ShuffleMapStage 35 (append at NativeMethodAccessorImpl.java:0) finished in 0.178 s\n",
      "25/05/01 06:32:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:48 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Registering RDD 51 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Got map stage job 18 (append at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 23.2 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fa5472e59cd7:35387 (size: 10.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[51] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Adding task set 37.0 with 8 tasks resource profile 0\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 3.0 in stage 37.0 (TID 130) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 5.0 in stage 37.0 (TID 131) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 7.0 in stage 37.0 (TID 132) (fa5472e59cd7, executor driver, partition 7, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 133) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 134) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 2.0 in stage 37.0 (TID 135) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 4.0 in stage 37.0 (TID 136) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 6.0 in stage 37.0 (TID 137) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO Executor: Running task 5.0 in stage 37.0 (TID 131)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 7.0 in stage 37.0 (TID 132)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 3.0 in stage 37.0 (TID 130)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 0.0 in stage 37.0 (TID 133)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 1.0 in stage 37.0 (TID 134)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 4.0 in stage 37.0 (TID 136)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 2.0 in stage 37.0 (TID 135)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 6.0 in stage 37.0 (TID 137)\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (14.8 KiB) non-empty blocks including 1 (14.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 12 (7.1 MiB) non-empty blocks including 12 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (6.9 KiB) non-empty blocks including 1 (6.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 4.0 in stage 37.0 (TID 136). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 2.0 in stage 37.0 (TID 135). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 0.0 in stage 37.0 (TID 133). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 6.0 in stage 37.0 (TID 137). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 4.0 in stage 37.0 (TID 136) in 6 ms on fa5472e59cd7 (executor driver) (1/8)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 1.0 in stage 37.0 (TID 134). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 2.0 in stage 37.0 (TID 135) in 7 ms on fa5472e59cd7 (executor driver) (2/8)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 133) in 7 ms on fa5472e59cd7 (executor driver) (3/8)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 6.0 in stage 37.0 (TID 137) in 7 ms on fa5472e59cd7 (executor driver) (4/8)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 134) in 7 ms on fa5472e59cd7 (executor driver) (5/8)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 3.0 in stage 37.0 (TID 130). 4261 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 3.0 in stage 37.0 (TID 130) in 15 ms on fa5472e59cd7 (executor driver) (6/8)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 7.0 in stage 37.0 (TID 132). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 7.0 in stage 37.0 (TID 132) in 17 ms on fa5472e59cd7 (executor driver) (7/8)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fa5472e59cd7:35387 in memory (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 5.0 in stage 37.0 (TID 131). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 5.0 in stage 37.0 (TID 131) in 175 ms on fa5472e59cd7 (executor driver) (8/8)\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:48 INFO DAGScheduler: ShuffleMapStage 37 (append at NativeMethodAccessorImpl.java:0) finished in 0.177 s\n",
      "25/05/01 06:32:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:48 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:48 INFO ShufflePartitionsUtil: For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 5.755792 ms\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Registering RDD 54 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Got map stage job 19 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Final stage: ShuffleMapStage 40 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[54] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[54] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Adding task set 40.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 138) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 139) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 2.0 in stage 40.0 (TID 140) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 3.0 in stage 40.0 (TID 141) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 4.0 in stage 40.0 (TID 142) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 5.0 in stage 40.0 (TID 143) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO Executor: Running task 0.0 in stage 40.0 (TID 138)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 2.0 in stage 40.0 (TID 140)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 3.0 in stage 40.0 (TID 141)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 1.0 in stage 40.0 (TID 139)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 4.0 in stage 40.0 (TID 142)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 5.0 in stage 40.0 (TID 143)\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1030.5 KiB) non-empty blocks including 3 (1030.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1199.7 KiB) non-empty blocks including 3 (1199.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1196.5 KiB) non-empty blocks including 3 (1196.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1896.1 KiB) non-empty blocks including 3 (1896.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 3 (1187.3 KiB) non-empty blocks including 3 (1187.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 2 (1247.6 KiB) non-empty blocks including 2 (1247.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 4.59675 ms\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 3.0 in stage 40.0 (TID 141). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 3.0 in stage 40.0 (TID 141) in 41 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 0.0 in stage 40.0 (TID 138). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 138) in 46 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 4.0 in stage 40.0 (TID 142). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 4.0 in stage 40.0 (TID 142) in 47 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 2.0 in stage 40.0 (TID 140). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 2.0 in stage 40.0 (TID 140) in 48 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 1.0 in stage 40.0 (TID 139). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 139) in 50 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 5.0 in stage 40.0 (TID 143). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 5.0 in stage 40.0 (TID 143) in 62 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:48 INFO DAGScheduler: ShuffleMapStage 40 (append at NativeMethodAccessorImpl.java:0) finished in 0.065 s\n",
      "25/05/01 06:32:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:48 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:48 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 4.229125 ms\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Registering RDD 57 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Got map stage job 20 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Final stage: ShuffleMapStage 44 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[57] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[57] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Adding task set 44.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 144) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 145) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 146) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 147) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 4.0 in stage 44.0 (TID 148) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 5.0 in stage 44.0 (TID 149) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO Executor: Running task 1.0 in stage 44.0 (TID 145)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 0.0 in stage 44.0 (TID 144)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 2.0 in stage 44.0 (TID 146)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 3.0 in stage 44.0 (TID 147)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 4.0 in stage 44.0 (TID 148)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 5.0 in stage 44.0 (TID 149)\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 4.941083 ms\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 3.0 in stage 44.0 (TID 147). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 147) in 148 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 2.0 in stage 44.0 (TID 146). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 1.0 in stage 44.0 (TID 145). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 146) in 194 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 145) in 194 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 4.0 in stage 44.0 (TID 148). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 4.0 in stage 44.0 (TID 148) in 195 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 0.0 in stage 44.0 (TID 144). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 144) in 195 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:48 INFO Executor: Finished task 5.0 in stage 44.0 (TID 149). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Finished task 5.0 in stage 44.0 (TID 149) in 205 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:48 INFO DAGScheduler: ShuffleMapStage 44 (append at NativeMethodAccessorImpl.java:0) finished in 0.207 s\n",
      "25/05/01 06:32:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:48 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:48 INFO ShufflePartitionsUtil: For shuffle(15), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 4.947291 ms\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Registering RDD 60 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Got map stage job 21 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Final stage: ShuffleMapStage 49 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[60] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:48 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:48 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:48 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[60] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:48 INFO TaskSchedulerImpl: Adding task set 49.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 150) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 151) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 2.0 in stage 49.0 (TID 152) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 3.0 in stage 49.0 (TID 153) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 4.0 in stage 49.0 (TID 154) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO TaskSetManager: Starting task 5.0 in stage 49.0 (TID 155) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:48 INFO Executor: Running task 1.0 in stage 49.0 (TID 151)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 2.0 in stage 49.0 (TID 152)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 0.0 in stage 49.0 (TID 150)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 5.0 in stage 49.0 (TID 155)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 3.0 in stage 49.0 (TID 153)\n",
      "25/05/01 06:32:48 INFO Executor: Running task 4.0 in stage 49.0 (TID 154)\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:48 INFO CodeGenerator: Code generated in 4.935708 ms\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 3.0 in stage 49.0 (TID 153). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 3.0 in stage 49.0 (TID 153) in 116 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 0.0 in stage 49.0 (TID 150). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 150) in 132 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 4.0 in stage 49.0 (TID 154). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 4.0 in stage 49.0 (TID 154) in 137 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 1.0 in stage 49.0 (TID 151). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 151) in 142 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 2.0 in stage 49.0 (TID 152). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 2.0 in stage 49.0 (TID 152) in 148 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 5.0 in stage 49.0 (TID 155). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 5.0 in stage 49.0 (TID 155) in 162 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:49 INFO DAGScheduler: ShuffleMapStage 49 (append at NativeMethodAccessorImpl.java:0) finished in 0.166 s\n",
      "25/05/01 06:32:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:49 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:49 INFO ShufflePartitionsUtil: For shuffle(16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 4.833625 ms\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Registering RDD 63 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 17\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Got map stage job 22 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Final stage: ShuffleMapStage 55 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[63] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 23.1 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fa5472e59cd7:35387 (size: 10.8 KiB, free: 434.3 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[63] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Adding task set 55.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 156) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 1.0 in stage 55.0 (TID 157) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 2.0 in stage 55.0 (TID 158) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 3.0 in stage 55.0 (TID 159) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 4.0 in stage 55.0 (TID 160) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 5.0 in stage 55.0 (TID 161) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO Executor: Running task 1.0 in stage 55.0 (TID 157)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 0.0 in stage 55.0 (TID 156)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 2.0 in stage 55.0 (TID 158)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 3.0 in stage 55.0 (TID 159)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 5.0 in stage 55.0 (TID 161)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 4.0 in stage 55.0 (TID 160)\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 4.323083 ms\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_25_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 3.0 in stage 55.0 (TID 159). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 3.0 in stage 55.0 (TID 159) in 84 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 0.0 in stage 55.0 (TID 156). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 1.0 in stage 55.0 (TID 157). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 4.0 in stage 55.0 (TID 160). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 156) in 118 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 1.0 in stage 55.0 (TID 157) in 118 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 4.0 in stage 55.0 (TID 160) in 118 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 2.0 in stage 55.0 (TID 158). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 2.0 in stage 55.0 (TID 158) in 119 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 5.0 in stage 55.0 (TID 161). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 5.0 in stage 55.0 (TID 161) in 130 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:49 INFO DAGScheduler: ShuffleMapStage 55 (append at NativeMethodAccessorImpl.java:0) finished in 0.133 s\n",
      "25/05/01 06:32:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:49 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:49 INFO ShufflePartitionsUtil: For shuffle(17), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_26_piece0 on fa5472e59cd7:35387 in memory (size: 10.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 5.975083 ms\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Registering RDD 66 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Got map stage job 23 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[66] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fa5472e59cd7:35387 (size: 10.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[66] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Adding task set 62.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 162) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 163) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 164) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 165) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 4.0 in stage 62.0 (TID 166) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 5.0 in stage 62.0 (TID 167) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO Executor: Running task 0.0 in stage 62.0 (TID 162)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 2.0 in stage 62.0 (TID 164)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 5.0 in stage 62.0 (TID 167)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 1.0 in stage 62.0 (TID 163)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 3.0 in stage 62.0 (TID 165)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 4.0 in stage 62.0 (TID 166)\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 3.864375 ms\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 3.0 in stage 62.0 (TID 165). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 165) in 94 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 2.0 in stage 62.0 (TID 164). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 1.0 in stage 62.0 (TID 163). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 164) in 123 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 4.0 in stage 62.0 (TID 166). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 163) in 123 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 4.0 in stage 62.0 (TID 166) in 122 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 0.0 in stage 62.0 (TID 162). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 162) in 127 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 5.0 in stage 62.0 (TID 167). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 5.0 in stage 62.0 (TID 167) in 134 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:49 INFO DAGScheduler: ShuffleMapStage 62 (append at NativeMethodAccessorImpl.java:0) finished in 0.138 s\n",
      "25/05/01 06:32:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:49 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:49 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 3.714416 ms\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Registering RDD 69 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Got map stage job 24 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Final stage: ShuffleMapStage 70 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 69)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting ShuffleMapStage 70 (MapPartitionsRDD[69] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 23.1 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.3 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[69] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Adding task set 70.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 168) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 1.0 in stage 70.0 (TID 169) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 2.0 in stage 70.0 (TID 170) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 3.0 in stage 70.0 (TID 171) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 4.0 in stage 70.0 (TID 172) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 5.0 in stage 70.0 (TID 173) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO Executor: Running task 0.0 in stage 70.0 (TID 168)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 2.0 in stage 70.0 (TID 170)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 1.0 in stage 70.0 (TID 169)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 4.0 in stage 70.0 (TID 172)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 5.0 in stage 70.0 (TID 173)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 3.0 in stage 70.0 (TID 171)\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 3.797583 ms\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 3.0 in stage 70.0 (TID 171). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 3.0 in stage 70.0 (TID 171) in 77 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 4.0 in stage 70.0 (TID 172). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 2.0 in stage 70.0 (TID 170). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 2.0 in stage 70.0 (TID 170) in 117 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 4.0 in stage 70.0 (TID 172) in 117 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 1.0 in stage 70.0 (TID 169). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 0.0 in stage 70.0 (TID 168). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 1.0 in stage 70.0 (TID 169) in 118 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 168) in 118 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_27_piece0 on fa5472e59cd7:35387 in memory (size: 10.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 5.0 in stage 70.0 (TID 173). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 5.0 in stage 70.0 (TID 173) in 132 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:49 INFO DAGScheduler: ShuffleMapStage 70 (append at NativeMethodAccessorImpl.java:0) finished in 0.135 s\n",
      "25/05/01 06:32:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:49 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:49 INFO ShufflePartitionsUtil: For shuffle(19), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 2.842875 ms\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Registering RDD 72 (append at NativeMethodAccessorImpl.java:0) as input to shuffle 20\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Got map stage job 25 (append at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Final stage: ShuffleMapStage 79 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[72] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fa5472e59cd7:35387 (size: 7.3 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[72] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Adding task set 79.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 174) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 1.0 in stage 79.0 (TID 175) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 2.0 in stage 79.0 (TID 176) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 3.0 in stage 79.0 (TID 177) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 4.0 in stage 79.0 (TID 178) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 5.0 in stage 79.0 (TID 179) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:49 INFO Executor: Running task 1.0 in stage 79.0 (TID 175)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 4.0 in stage 79.0 (TID 178)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 5.0 in stage 79.0 (TID 179)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 0.0 in stage 79.0 (TID 174)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 3.0 in stage 79.0 (TID 177)\n",
      "25/05/01 06:32:49 INFO Executor: Running task 2.0 in stage 79.0 (TID 176)\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO CodeGenerator: Code generated in 2.848166 ms\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 3.0 in stage 79.0 (TID 177). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 3.0 in stage 79.0 (TID 177) in 143 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 0.0 in stage 79.0 (TID 174). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 174) in 158 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_28_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 2.0 in stage 79.0 (TID 176). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 4.0 in stage 79.0 (TID 178). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 1.0 in stage 79.0 (TID 175). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 2.0 in stage 79.0 (TID 176) in 162 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 4.0 in stage 79.0 (TID 178) in 161 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 1.0 in stage 79.0 (TID 175) in 162 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:49 INFO Executor: Finished task 5.0 in stage 79.0 (TID 179). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Finished task 5.0 in stage 79.0 (TID 179) in 174 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:49 INFO DAGScheduler: ShuffleMapStage 79 (append at NativeMethodAccessorImpl.java:0) finished in 0.179 s\n",
      "25/05/01 06:32:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:49 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:49 INFO ShufflePartitionsUtil: For shuffle(20), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fa5472e59cd7:35387 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 30 from broadcast at SparkWrite.java:193\n",
      "25/05/01 06:32:49 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET). The input RDD has 1 partitions.\n",
      "25/05/01 06:32:49 INFO SparkContext: Starting job: append at NativeMethodAccessorImpl.java:0\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Got job 26 (append at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Final stage: ResultStage 89 (append at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting ResultStage 89 (ShuffledRowRDD[73] at append at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 10.6 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fa5472e59cd7:35387 (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (ShuffledRowRDD[73] at append at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/01 06:32:49 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0\n",
      "25/05/01 06:32:49 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 180) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/05/01 06:32:49 INFO Executor: Running task 0.0 in stage 89.0 (TID 180)\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Getting 6 (7.3 MiB) non-empty blocks including 6 (7.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_29_piece0 on fa5472e59cd7:35387 in memory (size: 7.3 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:49 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "25/05/01 06:32:49 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fa5472e59cd7:35387 in memory (size: 10.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/01 06:32:50 INFO DataWritingSparkTask: Committed partition 0 (task 180, attempt 0, stage 89.0)\n",
      "25/05/01 06:32:50 INFO Executor: Finished task 0.0 in stage 89.0 (TID 180). 16450 bytes result sent to driver\n",
      "25/05/01 06:32:50 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 180) in 370 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/05/01 06:32:50 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:50 INFO DAGScheduler: ResultStage 89 (append at NativeMethodAccessorImpl.java:0) finished in 0.371 s\n",
      "25/05/01 06:32:50 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/01 06:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished\n",
      "25/05/01 06:32:50 INFO DAGScheduler: Job 26 finished: append at NativeMethodAccessorImpl.java:0, took 0.373492 s\n",
      "25/05/01 06:32:50 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) is committing.\n",
      "25/05/01 06:32:50 INFO SparkWrite: Committing append with 9 new data files to table rest.default.flow_records\n",
      "25/05/01 06:32:50 INFO SnapshotProducer: Committed snapshot 8269785657131674967 (MergeAppend)\n",
      "25/05/01 06:32:50 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=rest.default.flow_records, snapshotId=8269785657131674967, sequenceNumber=5, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.090682709S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=9}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=18}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, addedDVs=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, removedDVs=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=504000}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=1008000}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3386110}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=5026731}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}, manifestsCreated=null, manifestsReplaced=null, manifestsKept=null, manifestEntriesProcessed=null}, metadata={engine-version=3.5.4, app-id=local-1746039683687, engine-name=spark, iceberg-version=Apache Iceberg unspecified (commit f39c1fa6f67a705a13dc2bc536f6002f38fc4cc7)}}\n",
      "25/05/01 06:32:50 INFO SparkWrite: Committed in 96 ms\n",
      "25/05/01 06:32:50 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=rest.default.flow_records, format=PARQUET) committed.\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df\n",
    "    .withColumn(\"bucket\", (hash(\"flow_id\") % 16).cast(\"int\"))\n",
    "    .repartition(\"network_id\", expr(\"date(created_at)\"), \"bucket\")\n",
    "    .drop(\"bucket\").drop(\"bucket_hint\")  # 🔥 MUST drop before writing to Iceberg\n",
    ")\n",
    "\n",
    "df.writeTo(\"default.flow_records\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571e059e-a1eb-48a5-add4-c68396d78f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 06:32:55 INFO DAGScheduler: Registering RDD 75 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Got map stage job 27 (showString at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Final stage: ShuffleMapStage 90 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[75] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:55 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 24.2 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:55 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:55 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fa5472e59cd7:35387 (size: 12.0 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:55 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[75] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/05/01 06:32:55 INFO TaskSchedulerImpl: Adding task set 90.0 with 12 tasks resource profile 0\n",
      "25/05/01 06:32:55 WARN TaskSetManager: Stage 90 contains a task of very large size (1462 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 181) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 1497901 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 1.0 in stage 90.0 (TID 182) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 1509353 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 2.0 in stage 90.0 (TID 183) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 1535205 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 3.0 in stage 90.0 (TID 184) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 4.0 in stage 90.0 (TID 185) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 1551375 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 5.0 in stage 90.0 (TID 186) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 1551473 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 6.0 in stage 90.0 (TID 187) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 1551307 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 7.0 in stage 90.0 (TID 188) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 1550943 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 8.0 in stage 90.0 (TID 189) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 1551251 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 9.0 in stage 90.0 (TID 190) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 1551325 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 10.0 in stage 90.0 (TID 191) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 1551405 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 11.0 in stage 90.0 (TID 192) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 1565214 bytes) \n",
      "25/05/01 06:32:55 INFO Executor: Running task 5.0 in stage 90.0 (TID 186)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 2.0 in stage 90.0 (TID 183)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 7.0 in stage 90.0 (TID 188)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 0.0 in stage 90.0 (TID 181)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 9.0 in stage 90.0 (TID 190)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 8.0 in stage 90.0 (TID 189)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 1.0 in stage 90.0 (TID 182)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 11.0 in stage 90.0 (TID 192)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 3.0 in stage 90.0 (TID 184)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 6.0 in stage 90.0 (TID 187)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 10.0 in stage 90.0 (TID 191)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 4.0 in stage 90.0 (TID 185)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 39, boot = -7517, init = 7531, finish = 25\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 7.0 in stage 90.0 (TID 188). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO BlockManagerInfo: Removed broadcast_31_piece0 on fa5472e59cd7:35387 in memory (size: 5.6 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 31, boot = -7530, init = 7531, finish = 30\n",
      "25/05/01 06:32:55 INFO BlockManagerInfo: Removed broadcast_30_piece0 on fa5472e59cd7:35387 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 7.0 in stage 90.0 (TID 188) in 134 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 4.0 in stage 90.0 (TID 185). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 4.0 in stage 90.0 (TID 185) in 137 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 43, boot = -7565, init = 7567, finish = 41\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 10.0 in stage 90.0 (TID 191). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 10.0 in stage 90.0 (TID 191) in 144 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 52, boot = -7571, init = 7595, finish = 28\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 42, boot = -7560, init = 7578, finish = 24\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 0.0 in stage 90.0 (TID 181). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 9.0 in stage 90.0 (TID 190). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 34, boot = -7559, init = 7560, finish = 33\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 181) in 163 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 9.0 in stage 90.0 (TID 190) in 155 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 38, boot = -7540, init = 7557, finish = 21\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 8.0 in stage 90.0 (TID 189). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 8.0 in stage 90.0 (TID 189) in 158 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 63, boot = -7533, init = 7539, finish = 57\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 43, boot = -7560, init = 7584, finish = 19\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 2.0 in stage 90.0 (TID 183). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 48, boot = -7534, init = 7553, finish = 29\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 3.0 in stage 90.0 (TID 184). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 11.0 in stage 90.0 (TID 192). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 2.0 in stage 90.0 (TID 183) in 167 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 3.0 in stage 90.0 (TID 184) in 163 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 6.0 in stage 90.0 (TID 187). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 11.0 in stage 90.0 (TID 192) in 160 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 6.0 in stage 90.0 (TID 187) in 161 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 60, boot = -7537, init = 7570, finish = 27\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 5.0 in stage 90.0 (TID 186). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 5.0 in stage 90.0 (TID 186) in 168 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/05/01 06:32:55 INFO PythonRunner: Times: total = 27, boot = -7566, init = 7567, finish = 26\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 1.0 in stage 90.0 (TID 182). 2185 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 1.0 in stage 90.0 (TID 182) in 174 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/05/01 06:32:55 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:55 INFO DAGScheduler: ShuffleMapStage 90 (showString at NativeMethodAccessorImpl.java:0) finished in 0.181 s\n",
      "25/05/01 06:32:55 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:55 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Registering RDD 78 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Got map stage job 28 (showString at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Final stage: ShuffleMapStage 92 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[78] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:55 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 23.2 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:55 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:55 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:55 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:55 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[78] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/05/01 06:32:55 INFO TaskSchedulerImpl: Adding task set 92.0 with 8 tasks resource profile 0\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 3.0 in stage 92.0 (TID 193) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 5.0 in stage 92.0 (TID 194) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 7.0 in stage 92.0 (TID 195) (fa5472e59cd7, executor driver, partition 7, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 196) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 1.0 in stage 92.0 (TID 197) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 2.0 in stage 92.0 (TID 198) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 4.0 in stage 92.0 (TID 199) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO TaskSetManager: Starting task 6.0 in stage 92.0 (TID 200) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:55 INFO Executor: Running task 3.0 in stage 92.0 (TID 193)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 7.0 in stage 92.0 (TID 195)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 5.0 in stage 92.0 (TID 194)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 6.0 in stage 92.0 (TID 200)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 0.0 in stage 92.0 (TID 196)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 4.0 in stage 92.0 (TID 199)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 1.0 in stage 92.0 (TID 197)\n",
      "25/05/01 06:32:55 INFO Executor: Running task 2.0 in stage 92.0 (TID 198)\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 1 (14.8 KiB) non-empty blocks including 1 (14.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 12 (7.1 MiB) non-empty blocks including 12 (7.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Getting 1 (6.9 KiB) non-empty blocks including 1 (6.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 6.0 in stage 92.0 (TID 200). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 1.0 in stage 92.0 (TID 197). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 0.0 in stage 92.0 (TID 196). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 2.0 in stage 92.0 (TID 198). 4003 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 4.0 in stage 92.0 (TID 199). 4046 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 6.0 in stage 92.0 (TID 200) in 5 ms on fa5472e59cd7 (executor driver) (1/8)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 1.0 in stage 92.0 (TID 197) in 6 ms on fa5472e59cd7 (executor driver) (2/8)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 196) in 6 ms on fa5472e59cd7 (executor driver) (3/8)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 4.0 in stage 92.0 (TID 199) in 7 ms on fa5472e59cd7 (executor driver) (4/8)\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 2.0 in stage 92.0 (TID 198) in 7 ms on fa5472e59cd7 (executor driver) (5/8)\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 3.0 in stage 92.0 (TID 193). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 3.0 in stage 92.0 (TID 193) in 9 ms on fa5472e59cd7 (executor driver) (6/8)\n",
      "25/05/01 06:32:55 INFO Executor: Finished task 7.0 in stage 92.0 (TID 195). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:55 INFO TaskSetManager: Finished task 7.0 in stage 92.0 (TID 195) in 10 ms on fa5472e59cd7 (executor driver) (7/8)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 92.0 (TID 194). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 92.0 (TID 194) in 145 ms on fa5472e59cd7 (executor driver) (8/8)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 92 (showString at NativeMethodAccessorImpl.java:0) finished in 0.147 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(22), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 81 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 23\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 29 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 95 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[81] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[81] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 95.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 201) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 202) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 95.0 (TID 203) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 95.0 (TID 204) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 95.0 (TID 205) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 95.0 (TID 206) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 95.0 (TID 202)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 95.0 (TID 204)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 95.0 (TID 201)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 95.0 (TID 203)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 95.0 (TID 205)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 95.0 (TID 206)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1187.3 KiB) non-empty blocks including 3 (1187.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1199.7 KiB) non-empty blocks including 3 (1199.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1030.5 KiB) non-empty blocks including 3 (1030.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1896.1 KiB) non-empty blocks including 3 (1896.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 3 (1196.5 KiB) non-empty blocks including 3 (1196.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 2 (1247.6 KiB) non-empty blocks including 2 (1247.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Removed broadcast_33_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 95.0 (TID 204). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 95.0 (TID 204) in 29 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 95.0 (TID 205). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 95.0 (TID 201). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 95.0 (TID 205) in 42 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 201) in 43 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 95.0 (TID 202). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 202) in 45 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 95.0 (TID 203). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 95.0 (TID 203) in 49 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 95.0 (TID 206). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 95.0 (TID 206) in 53 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 95 (showString at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(23), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 84 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 24\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 30 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 99 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 98)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[84] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[84] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 99.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 207) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 99.0 (TID 208) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 99.0 (TID 209) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 99.0 (TID 210) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 99.0 (TID 211) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 99.0 (TID 212) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 99.0 (TID 207)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 99.0 (TID 208)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 99.0 (TID 211)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 99.0 (TID 210)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 99.0 (TID 212)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 99.0 (TID 209)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 99.0 (TID 210). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 99.0 (TID 210) in 28 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 99.0 (TID 211). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 99.0 (TID 211) in 30 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 99.0 (TID 208). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 99.0 (TID 208) in 33 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 99.0 (TID 207). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 207) in 34 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 99.0 (TID 209). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 99.0 (TID 209) in 35 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 99.0 (TID 212). 4175 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 99.0 (TID 212) in 43 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 99 (showString at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(24), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 87 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 25\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 31 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 104 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 104 (MapPartitionsRDD[87] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 104 (MapPartitionsRDD[87] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 104.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 213) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 214) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 104.0 (TID 215) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 104.0 (TID 216) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 104.0 (TID 217) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 104.0 (TID 218) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 104.0 (TID 213)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 104.0 (TID 215)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 104.0 (TID 216)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 104.0 (TID 214)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 104.0 (TID 217)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 104.0 (TID 218)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Removed broadcast_35_piece0 on fa5472e59cd7:35387 in memory (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 104.0 (TID 216). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 104.0 (TID 216) in 28 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 104.0 (TID 215). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 104.0 (TID 217). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 104.0 (TID 217) in 30 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 104.0 (TID 215) in 30 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 104.0 (TID 213). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 213) in 32 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 104.0 (TID 214). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 214) in 37 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 104.0 (TID 218). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 104.0 (TID 218) in 42 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 104 (showString at NativeMethodAccessorImpl.java:0) finished in 0.044 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 90 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 26\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 32 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 110 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 109)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[90] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 23.1 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.4 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[90] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 110.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 219) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 220) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 110.0 (TID 221) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 110.0 (TID 222) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 110.0 (TID 223) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 110.0 (TID 224) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 110.0 (TID 219)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 110.0 (TID 221)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 110.0 (TID 220)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 110.0 (TID 222)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 110.0 (TID 223)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 110.0 (TID 224)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 110.0 (TID 222). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 110.0 (TID 222) in 29 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 110.0 (TID 219). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 219) in 30 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 110.0 (TID 223). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 110.0 (TID 223) in 32 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 110.0 (TID 220). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 110.0 (TID 221). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 220) in 33 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 110.0 (TID 221) in 33 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 110.0 (TID 224). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 110.0 (TID 224) in 43 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 110 (showString at NativeMethodAccessorImpl.java:0) finished in 0.045 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(26), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 93 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 27\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 33 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 117 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 116)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 117 (MapPartitionsRDD[93] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 23.1 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 117 (MapPartitionsRDD[93] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 117.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 225) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 117.0 (TID 226) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 117.0 (TID 227) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 117.0 (TID 228) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 117.0 (TID 229) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 117.0 (TID 230) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 117.0 (TID 225)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 117.0 (TID 228)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 117.0 (TID 229)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 117.0 (TID 230)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 117.0 (TID 227)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 117.0 (TID 226)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 117.0 (TID 228). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 117.0 (TID 228) in 27 ms on fa5472e59cd7 (executor driver) (1/6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------------------+----------+\n",
      "|      flow_id|network_id|         created_at|bytes_sent|\n",
      "+-------------+----------+-------------------+----------+\n",
      "| flow_net9_59|      net9|2025-04-30 11:00:00|     42246|\n",
      "| flow_net9_67|      net9|2025-04-30 19:00:00|     71074|\n",
      "|flow_net9_183|      net9|2025-04-30 15:00:00|     84031|\n",
      "|flow_net9_193|      net9|2025-04-30 01:00:00|      8733|\n",
      "|flow_net9_219|      net9|2025-04-30 03:00:00|      6481|\n",
      "|flow_net9_221|      net9|2025-04-30 05:00:00|     59891|\n",
      "|flow_net9_272|      net9|2025-04-30 08:00:00|     42020|\n",
      "|flow_net9_274|      net9|2025-04-30 10:00:00|     25452|\n",
      "|flow_net9_350|      net9|2025-04-30 14:00:00|     43967|\n",
      "|flow_net9_381|      net9|2025-04-30 21:00:00|     41857|\n",
      "|flow_net9_397|      net9|2025-04-30 13:00:00|     52746|\n",
      "|flow_net9_419|      net9|2025-04-30 11:00:00|     24289|\n",
      "|flow_net9_484|      net9|2025-04-30 04:00:00|     37136|\n",
      "| flow_net1_20|      net1|2025-04-30 20:00:00|     59928|\n",
      "|flow_net1_113|      net1|2025-04-30 17:00:00|     50601|\n",
      "|flow_net1_150|      net1|2025-04-30 06:00:00|     46178|\n",
      "|flow_net1_156|      net1|2025-04-30 12:00:00|     67862|\n",
      "|flow_net1_167|      net1|2025-04-30 23:00:00|     32025|\n",
      "|flow_net1_206|      net1|2025-04-30 14:00:00|     97454|\n",
      "|flow_net1_247|      net1|2025-04-30 07:00:00|     15644|\n",
      "+-------------+----------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 117.0 (TID 227). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 117.0 (TID 225). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 117.0 (TID 227) in 36 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 225) in 38 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 117.0 (TID 229). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 117.0 (TID 229) in 38 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 117.0 (TID 226). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 117.0 (TID 226) in 39 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 117.0 (TID 230). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 117.0 (TID 230) in 47 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 117 (showString at NativeMethodAccessorImpl.java:0) finished in 0.049 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(27), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Registering RDD 96 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 28\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got map stage job 34 (showString at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ShuffleMapStage 125 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ShuffleMapStage 125 (MapPartitionsRDD[96] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 23.1 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on fa5472e59cd7:35387 (size: 10.9 KiB, free: 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 125 (MapPartitionsRDD[96] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 125.0 with 6 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 231) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 1.0 in stage 125.0 (TID 232) (fa5472e59cd7, executor driver, partition 1, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 2.0 in stage 125.0 (TID 233) (fa5472e59cd7, executor driver, partition 2, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 3.0 in stage 125.0 (TID 234) (fa5472e59cd7, executor driver, partition 3, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 4.0 in stage 125.0 (TID 235) (fa5472e59cd7, executor driver, partition 4, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 5.0 in stage 125.0 (TID 236) (fa5472e59cd7, executor driver, partition 5, NODE_LOCAL, 9196 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 125.0 (TID 231)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 2.0 in stage 125.0 (TID 233)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 1.0 in stage 125.0 (TID 232)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 5.0 in stage 125.0 (TID 236)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 3.0 in stage 125.0 (TID 234)\n",
      "25/05/01 06:32:56 INFO Executor: Running task 4.0 in stage 125.0 (TID 235)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1247.1 KiB) non-empty blocks including 1 (1247.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1194.8 KiB) non-empty blocks including 1 (1194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1894.6 KiB) non-empty blocks including 1 (1894.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1028.9 KiB) non-empty blocks including 1 (1028.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1196.4 KiB) non-empty blocks including 1 (1196.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 3.0 in stage 125.0 (TID 234). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 3.0 in stage 125.0 (TID 234) in 27 ms on fa5472e59cd7 (executor driver) (1/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 4.0 in stage 125.0 (TID 235). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 4.0 in stage 125.0 (TID 235) in 32 ms on fa5472e59cd7 (executor driver) (2/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 1.0 in stage 125.0 (TID 232). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 1.0 in stage 125.0 (TID 232) in 35 ms on fa5472e59cd7 (executor driver) (3/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 125.0 (TID 231). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 231) in 37 ms on fa5472e59cd7 (executor driver) (4/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 2.0 in stage 125.0 (TID 233). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 2.0 in stage 125.0 (TID 233) in 38 ms on fa5472e59cd7 (executor driver) (5/6)\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 5.0 in stage 125.0 (TID 236). 4218 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 5.0 in stage 125.0 (TID 236) in 43 ms on fa5472e59cd7 (executor driver) (6/6)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ShuffleMapStage 125 (showString at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/01 06:32:56 INFO DAGScheduler: running: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: waiting: Set()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: failed: Set()\n",
      "25/05/01 06:32:56 INFO ShufflePartitionsUtil: For shuffle(28), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/05/01 06:32:56 INFO CodeGenerator: Code generated in 3.720542 ms\n",
      "25/05/01 06:32:56 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Got job 35 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Final stage: ResultStage 134 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 133)\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[99] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
      "25/05/01 06:32:56 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on fa5472e59cd7:35387 (size: 6.4 KiB, free: 434.3 MiB)\n",
      "25/05/01 06:32:56 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[99] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks resource profile 0\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 237) (fa5472e59cd7, executor driver, partition 0, NODE_LOCAL, 9207 bytes) \n",
      "25/05/01 06:32:56 INFO Executor: Running task 0.0 in stage 134.0 (TID 237)\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (1185.0 KiB) non-empty blocks including 1 (1185.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/01 06:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/01 06:32:56 INFO CodeGenerator: Code generated in 4.58975 ms\n",
      "25/05/01 06:32:56 INFO Executor: Finished task 0.0 in stage 134.0 (TID 237). 3631 bytes result sent to driver\n",
      "25/05/01 06:32:56 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 237) in 10 ms on fa5472e59cd7 (executor driver) (1/1)\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool \n",
      "25/05/01 06:32:56 INFO DAGScheduler: ResultStage 134 (showString at NativeMethodAccessorImpl.java:0) finished in 0.012 s\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/01 06:32:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished\n",
      "25/05/01 06:32:56 INFO DAGScheduler: Job 35 finished: showString at NativeMethodAccessorImpl.java:0, took 0.015626 s\n",
      "25/05/01 06:32:56 INFO CodeGenerator: Code generated in 3.23975 ms\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a30b6e-a47e-4766-b72a-7bd872868f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[flow_id: string, network_id: string, created_at: timestamp, bytes_sent: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"bucket_hint\").drop(\"bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2b4e9bb-ca54-4860-9f49-e50da2b4a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 11:27:14 INFO RESTSessionCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "25/05/01 11:27:14 INFO RESTSessionCatalog: Table properties enforced at catalog level through catalog properties: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE default.flow_records_test_identity (\n",
    "  flow_id STRING,\n",
    "  network_id STRING,\n",
    "  created_at TIMESTAMP,\n",
    "  bytes_sent BIGINT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (\n",
    "  network_id,\n",
    "  days(created_at)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e5617fa-afda-4c86-a403-6e087cca4a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 12:05:14 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodeGenerator: Code generated in 15.539709 ms\n",
      "25/05/04 12:05:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Got job 36 (parquet at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Final stage: ResultStage 135 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[103] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/04 12:05:14 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 225.4 KiB, free 434.2 MiB)\n",
      "25/05/04 12:05:14 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 81.3 KiB, free 434.1 MiB)\n",
      "25/05/04 12:05:14 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on fa5472e59cd7:35387 (size: 81.3 KiB, free: 434.3 MiB)\n",
      "25/05/04 12:05:14 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/04 12:05:14 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 135 (MapPartitionsRDD[103] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "25/05/04 12:05:14 INFO TaskSchedulerImpl: Adding task set 135.0 with 12 tasks resource profile 0\n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 238) (fa5472e59cd7, executor driver, partition 0, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 1.0 in stage 135.0 (TID 239) (fa5472e59cd7, executor driver, partition 1, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 2.0 in stage 135.0 (TID 240) (fa5472e59cd7, executor driver, partition 2, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 3.0 in stage 135.0 (TID 241) (fa5472e59cd7, executor driver, partition 3, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 4.0 in stage 135.0 (TID 242) (fa5472e59cd7, executor driver, partition 4, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 5.0 in stage 135.0 (TID 243) (fa5472e59cd7, executor driver, partition 5, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 6.0 in stage 135.0 (TID 244) (fa5472e59cd7, executor driver, partition 6, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 7.0 in stage 135.0 (TID 245) (fa5472e59cd7, executor driver, partition 7, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 8.0 in stage 135.0 (TID 246) (fa5472e59cd7, executor driver, partition 8, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 9.0 in stage 135.0 (TID 247) (fa5472e59cd7, executor driver, partition 9, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 10.0 in stage 135.0 (TID 248) (fa5472e59cd7, executor driver, partition 10, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO TaskSetManager: Starting task 11.0 in stage 135.0 (TID 249) (fa5472e59cd7, executor driver, partition 11, PROCESS_LOCAL, 9332 bytes) \n",
      "25/05/04 12:05:14 INFO Executor: Running task 0.0 in stage 135.0 (TID 238)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 2.0 in stage 135.0 (TID 240)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 3.0 in stage 135.0 (TID 241)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 1.0 in stage 135.0 (TID 239)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 4.0 in stage 135.0 (TID 242)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 5.0 in stage 135.0 (TID 243)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 7.0 in stage 135.0 (TID 245)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 6.0 in stage 135.0 (TID 244)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 8.0 in stage 135.0 (TID 246)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 10.0 in stage 135.0 (TID 248)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 9.0 in stage 135.0 (TID 247)\n",
      "25/05/04 12:05:14 INFO Executor: Running task 11.0 in stage 135.0 (TID 249)\n",
      "25/05/04 12:05:14 INFO CodeGenerator: Code generated in 8.257083 ms\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/05/04 12:05:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO CodecConfig: Compression: SNAPPY\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"flow_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"network_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"bytes_sent\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required binary flow_id (STRING);\n",
      "  optional binary network_id (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 bytes_sent;\n",
      "}\n",
      "\n",
      "       \n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:14 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "25/05/04 12:05:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/05/04 12:05:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/05/04 12:05:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/05/04 12:05:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/05/04 12:05:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/05/04 12:05:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/05/04 12:05:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/05/04 12:05:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/05/04 12:05:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000008_246' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000008\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000008_246: Committed. Elapsed time: 5 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000011_249' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000011\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000011_249: Committed. Elapsed time: 6 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000002_240' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000002\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000002_240: Committed. Elapsed time: 6 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000004_242' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000004\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000004_242: Committed. Elapsed time: 7 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000003_241' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000003\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000003_241: Committed. Elapsed time: 7 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000007_245' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000007\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000007_245: Committed. Elapsed time: 5 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000005_243' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000005\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000005_243: Committed. Elapsed time: 8 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000009_247' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000009\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000009_247: Committed. Elapsed time: 8 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000001_239' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000001\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000001_239: Committed. Elapsed time: 9 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000000_238' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000000\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000000_238: Committed. Elapsed time: 7 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000006_244' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000006\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000006_244: Committed. Elapsed time: 11 ms.\n",
      "25/05/04 12:05:22 INFO FileOutputCommitter: Saved output of task 'attempt_202505041205149068625840664559413_0135_m_000010_248' to file:/opt/spark/home/iceberg/datasets/flow_50m/_temporary/0/task_202505041205149068625840664559413_0135_m_000010\n",
      "25/05/04 12:05:22 INFO SparkHadoopMapRedUtil: attempt_202505041205149068625840664559413_0135_m_000010_248: Committed. Elapsed time: 12 ms.\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 0.0 in stage 135.0 (TID 238). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 7.0 in stage 135.0 (TID 245). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 3.0 in stage 135.0 (TID 241). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 5.0 in stage 135.0 (TID 243). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 4.0 in stage 135.0 (TID 242). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 1.0 in stage 135.0 (TID 239). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 8.0 in stage 135.0 (TID 246). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 6.0 in stage 135.0 (TID 244). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 2.0 in stage 135.0 (TID 240). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 11.0 in stage 135.0 (TID 249). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 10.0 in stage 135.0 (TID 248). 2527 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO Executor: Finished task 9.0 in stage 135.0 (TID 247). 2570 bytes result sent to driver\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 238) in 7846 ms on fa5472e59cd7 (executor driver) (1/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 7.0 in stage 135.0 (TID 245) in 7841 ms on fa5472e59cd7 (executor driver) (2/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 3.0 in stage 135.0 (TID 241) in 7841 ms on fa5472e59cd7 (executor driver) (3/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 4.0 in stage 135.0 (TID 242) in 7841 ms on fa5472e59cd7 (executor driver) (4/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 5.0 in stage 135.0 (TID 243) in 7841 ms on fa5472e59cd7 (executor driver) (5/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 1.0 in stage 135.0 (TID 239) in 7841 ms on fa5472e59cd7 (executor driver) (6/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 6.0 in stage 135.0 (TID 244) in 7841 ms on fa5472e59cd7 (executor driver) (7/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 11.0 in stage 135.0 (TID 249) in 7840 ms on fa5472e59cd7 (executor driver) (8/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 10.0 in stage 135.0 (TID 248) in 7840 ms on fa5472e59cd7 (executor driver) (9/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 2.0 in stage 135.0 (TID 240) in 7841 ms on fa5472e59cd7 (executor driver) (10/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 9.0 in stage 135.0 (TID 247) in 7841 ms on fa5472e59cd7 (executor driver) (11/12)\n",
      "25/05/04 12:05:22 INFO TaskSetManager: Finished task 8.0 in stage 135.0 (TID 246) in 7841 ms on fa5472e59cd7 (executor driver) (12/12)\n",
      "25/05/04 12:05:22 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "25/05/04 12:05:22 INFO DAGScheduler: ResultStage 135 (parquet at NativeMethodAccessorImpl.java:0) finished in 7.894 s\n",
      "25/05/04 12:05:22 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/04 12:05:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished\n",
      "25/05/04 12:05:22 INFO DAGScheduler: Job 36 finished: parquet at NativeMethodAccessorImpl.java:0, took 7.896225 s\n",
      "25/05/04 12:05:22 INFO FileFormatWriter: Start to commit write Job 75803c73-58db-4214-a0e2-2025f575411b.\n",
      "25/05/04 12:05:22 INFO FileFormatWriter: Write Job 75803c73-58db-4214-a0e2-2025f575411b committed. Elapsed time: 30 ms.\n",
      "25/05/04 12:05:22 INFO FileFormatWriter: Finished processing stats for write job 75803c73-58db-4214-a0e2-2025f575411b.\n",
      "25/05/04 12:21:54 INFO BlockManagerInfo: Removed broadcast_41_piece0 on fa5472e59cd7:35387 in memory (size: 81.3 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, concat_ws, lit, floor, rand\n",
    "from datetime import datetime\n",
    "\n",
    "# 50 million rows\n",
    "N = 50_000_000\n",
    "\n",
    "df = (\n",
    "    spark.range(N)\n",
    "    .withColumn(\"network_id\", expr(\"CASE WHEN id % 10 = 0 THEN 'net1' ELSE concat('net', (id % 10)) END\"))\n",
    "    .withColumn(\"flow_id\", concat_ws(\"_\", lit(\"flow\"), col(\"network_id\"), col(\"id\")))\n",
    "    .withColumn(\"created_at\", expr(\"timestamp_seconds(1714435200 + (id % (60*60*24)))\"))  # random seconds on 2025-04-30\n",
    "    .withColumn(\"bytes_sent\", (rand() * 100000 + 1000).cast(\"long\"))\n",
    "    .select(\"flow_id\", \"network_id\", \"created_at\", \"bytes_sent\")\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"/opt/spark/home/iceberg/datasets/flow_50m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46cf5dc8-b103-487e-9270-20998a0b79c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fa5472e59cd7:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iceberg-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff7353b700>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33757f4d-a89f-40fa-a025-8a8d638eb998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
